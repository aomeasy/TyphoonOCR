import streamlit as st
import requests
import base64
import json
from io import BytesIO
from PIL import Image
import PyPDF2
import tempfile
import os
import sys
import subprocess
from typing import Optional, Dict, Any, List, Tuple
import traceback
import hashlib
import pickle
import sqlite3
from datetime import datetime
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import markdown
import re

# Try importing pdf2image with error handling
try:
    import pdf2image
    PDF2IMAGE_AVAILABLE = True
except ImportError:
    PDF2IMAGE_AVAILABLE = False
    st.warning("‚ö†Ô∏è pdf2image not available. PDF conversion will use fallback method.")

# Page configuration
st.set_page_config(
    page_title="üå™Ô∏è Typhoon OCR with RAG",
    page_icon="üå™Ô∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better styling
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 15px;
        text-align: center;
        color: white;
        margin-bottom: 2rem;
    }
    
    .feature-card {
        background: #f8f9fa;
        padding: 1.5rem;
        border-radius: 10px;
        border-left: 4px solid #667eea;
        margin: 1rem 0;
    }
    
    .model-card {
        background: linear-gradient(45deg, #e3f2fd, #bbdefb);
        padding: 1rem;
        border-radius: 10px;
        border: 2px solid #2196f3;
        text-align: center;
        margin: 0.5rem;
    }
    
    .success-message {
        background: #d4edda;
        color: #155724;
        padding: 1rem;
        border-radius: 5px;
        border: 1px solid #c3e6cb;
    }
    
    .error-message {
        background: #f8d7da;
        color: #721c24;
        padding: 1rem;
        border-radius: 5px;
        border: 1px solid #f5c6cb;
    }
    
    .warning-message {
        background: #fff3cd;
        color: #856404;
        padding: 1rem;
        border-radius: 5px;
        border: 1px solid #ffeaa7;
    }
    
    .chat-message {
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
    }
    
    .user-message {
        background: #e3f2fd;
        border-left: 4px solid #2196f3;
    }
    
    .assistant-message {
        background: #f3e5f5;
        border-left: 4px solid #9c27b0;
    }
    
    .knowledge-card {
        background: #e8f5e8;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #4caf50;
        margin: 0.5rem 0;
    }
    
    .source-reference {
        background: #fff3cd;
        border-left: 4px solid #ffc107;
        padding: 0.8rem;
        margin: 0.3rem 0;
        border-radius: 5px;
        font-size: 0.9em;
    }
    
    .stButton > button {
        background: linear-gradient(45deg, #667eea, #764ba2) !important;
        color: white !important;
        border: none !important;
        border-radius: 25px !important;
        padding: 0.5rem 2rem !important;
    }
</style>
""", unsafe_allow_html=True)

# Configuration
EMBEDDING_API_URL = "http://209.15.123.47:11434/api/embeddings"
EMBEDDING_MODEL = "nomic-embed-text:latest"
OLLAMA_API_URL = "http://209.15.123.47:11434/api/generate"

# Initialize session state for RAG
if "knowledge_base" not in st.session_state:
    st.session_state.knowledge_base = []
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "rag_db_path" not in st.session_state:
    st.session_state.rag_db_path = "typhoon_rag_knowledge.db"

AVAILABLE_MODELS = {
    "scb10x/typhoon-ocr-7b:latest": {
        "name": "Typhoon OCR 7B",
        "description": "‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç OCR ‡πÑ‡∏ó‡∏¢-‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©",
        "icon": "üå™Ô∏è",
        "best_for": "OCR, Document parsing, Thai-English text"
    },
    "qwen2.5:14b": {
        "name": "Qwen2.5 14B", 
        "description": "‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà",
        "icon": "üß†",
        "best_for": "General purpose, Complex reasoning"
    },
    "scb10x/llama3.1-typhoon2-8b-instruct:latest": {
        "name": "Typhoon2 8B",
        "description": "‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î", 
        "icon": "üáπüá≠",
        "best_for": "Thai language, Instructions following"
    },
    "nomic-embed-text:latest": {
        "name": "Nomic Embed Text",
        "description": "‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Embeddings",
        "icon": "üîó",
        "best_for": "Text embeddings, Semantic search"
    }
}

# ==================== ENHANCED RAG SYSTEM FUNCTIONS ====================

def mmr_search(query_embedding: List[float], documents: List[Tuple], 
               top_k: int = 5, diversity_weight: float = 0.3) -> List[Tuple]:
    """
    Maximal Marginal Relevance search for balanced relevance and diversity
    """
    if not documents:
        return []
    
    # Calculate similarity scores
    doc_embeddings = [pickle.loads(doc[2]) for doc in documents]  # embedding is at index 2
    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]
    
    # Initialize result set
    selected_docs = []
    remaining_docs = list(enumerate(zip(documents, similarities)))
    
    # Select first document (highest similarity)
    if remaining_docs:
        best_idx = max(remaining_docs, key=lambda x: x[1][1])[0]
        selected_docs.append(remaining_docs[best_idx][1][0])
        remaining_docs = [doc for i, doc in remaining_docs if i != best_idx]
    
    # Select remaining documents with MMR
    while len(selected_docs) < top_k and remaining_docs:
        best_score = -1
        best_idx = -1
        
        for i, (doc, sim) in remaining_docs:
            # Calculate diversity score (minimum similarity to already selected docs)
            selected_embeddings = [pickle.loads(selected_doc[2]) for selected_doc in selected_docs]
            doc_embedding = pickle.loads(doc[2])
            
            if selected_embeddings:
                diversity_scores = cosine_similarity([doc_embedding], selected_embeddings)[0]
                diversity = 1 - max(diversity_scores)  # 1 - max similarity = diversity
            else:
                diversity = 1.0
            
            # MMR score: balance between relevance and diversity
            mmr_score = (1 - diversity_weight) * sim + diversity_weight * diversity
            
            if mmr_score > best_score:
                best_score = mmr_score
                best_idx = i
        
        if best_idx != -1:
            selected_docs.append(remaining_docs[best_idx][1][0])
            remaining_docs = [doc for i, doc in remaining_docs if i != best_idx]
        else:
            break
    
    return selected_docs

class RAGKnowledgeBase:
    def __init__(self, db_path: str = "typhoon_rag_knowledge.db"):
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """Initialize SQLite database for knowledge storage"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS documents (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                filename TEXT NOT NULL,
                content TEXT NOT NULL,
                embedding BLOB,
                chunk_id INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                file_hash TEXT,
                metadata TEXT,
                header_level INTEGER DEFAULT 0,
                section_title TEXT DEFAULT ''
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS chat_sessions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_id TEXT,
                question TEXT,
                answer TEXT,
                context TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def get_embedding(self, text: str) -> Optional[List[float]]:
        """Get embedding from Ollama API"""
        try:
            response = requests.post(
                EMBEDDING_API_URL,
                json={
                    "model": EMBEDDING_MODEL,
                    "prompt": text
                },
                timeout=60
            )
            response.raise_for_status()
            result = response.json()
            return result.get('embedding', [])
        except Exception as e:
            st.error(f"‚ùå Error getting embedding: {str(e)}")
            return None
    
    def chunk_markdown(self, content: str) -> List[Dict[str, Any]]:
        """
        Smart markdown chunking that preserves headers and context
        """
        chunks = []
        lines = content.split('\n')
        
        current_chunk = ""
        current_headers = []
        chunk_id = 0
        
        for line in lines:
            # Check if line is a header
            header_match = re.match(r'^(#{1,6})\s+(.+)$', line.strip())
            
            if header_match:
                header_level = len(header_match.group(1))
                header_text = header_match.group(2)
                
                # Save previous chunk if it exists and has content
                if current_chunk.strip():
                    chunks.append({
                        'content': current_chunk.strip(),
                        'chunk_id': chunk_id,
                        'headers': current_headers.copy(),
                        'section_title': current_headers[-1] if current_headers else ''
                    })
                    chunk_id += 1
                
                # Update headers hierarchy
                current_headers = current_headers[:header_level-1] + [header_text]
                current_chunk = line + '\n'
                
            else:
                current_chunk += line + '\n'
                
                # If chunk gets too long, split it but preserve context
                if len(current_chunk) > 1500:  # Adjustable chunk size
                    # Try to find a good break point
                    sentences = current_chunk.split('. ')
                    if len(sentences) > 1:
                        # Take most sentences, leave some overlap
                        break_point = len(sentences) * 2 // 3
                        chunk_content = '. '.join(sentences[:break_point]) + '.'
                        remaining_content = '. '.join(sentences[break_point:])
                        
                        chunks.append({
                            'content': chunk_content.strip(),
                            'chunk_id': chunk_id,
                            'headers': current_headers.copy(),
                            'section_title': current_headers[-1] if current_headers else ''
                        })
                        chunk_id += 1
                        
                        # Start new chunk with header context
                        header_context = '\n'.join([f"{'#' * (i+1)} {h}" for i, h in enumerate(current_headers)])
                        current_chunk = header_context + '\n\n' + remaining_content
        
        # Add final chunk if it has content
        if current_chunk.strip():
            chunks.append({
                'content': current_chunk.strip(),
                'chunk_id': chunk_id,
                'headers': current_headers.copy(),
                'section_title': current_headers[-1] if current_headers else ''
            })
        
        return chunks
    
    def chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
        """Split text into overlapping chunks (for non-markdown content)"""
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            # Try to break at sentence boundaries
            if end < len(text):
                # Look for sentence endings
                sentence_ends = ['. ', '! ', '? ', '\n\n', '„ÄÇ', 'ÔºÅ', 'Ôºü']
                best_break = end
                
                for i in range(min(100, len(text) - end)):
                    for ending in sentence_ends:
                        if text[end + i:end + i + len(ending)] == ending:
                            best_break = end + i + len(ending)
                            break
                    if best_break != end:
                        break
                
                end = best_break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end - overlap
            if start >= len(text):
                break
                
        return chunks
    
    def add_document(self, filename: str, content: str, metadata: Dict = None) -> bool:
        """Add document to knowledge base with enhanced chunking"""
        try:
            # Create file hash for deduplication
            file_hash = hashlib.md5(content.encode()).hexdigest()
            
            # Check if document already exists
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT id FROM documents WHERE file_hash = ?", (file_hash,))
            if cursor.fetchone():
                conn.close()
                st.warning(f"‚ö†Ô∏è Document {filename} already exists in knowledge base")
                return False
            
            # Choose chunking strategy based on file type
            if filename.endswith('.md'):
                # Use smart markdown chunking
                chunks_data = self.chunk_markdown(content)
                chunk_type = "markdown"
            else:
                # Use regular text chunking
                plain_chunks = self.chunk_text(content)
                chunks_data = [
                    {
                        'content': chunk,
                        'chunk_id': i,
                        'headers': [],
                        'section_title': ''
                    }
                    for i, chunk in enumerate(plain_chunks)
                ]
                chunk_type = "text"
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for i, chunk_data in enumerate(chunks_data):
                status_text.text(f"Processing {chunk_type} chunk {i+1}/{len(chunks_data)}...")
                progress_bar.progress((i + 1) / len(chunks_data))
                
                # Get embedding
                embedding = self.get_embedding(chunk_data['content'])
                if embedding:
                    # Store in database with enhanced metadata
                    chunk_metadata = metadata.copy() if metadata else {}
                    chunk_metadata.update({
                        'headers': chunk_data['headers'],
                        'chunk_type': chunk_type
                    })
                    
                    cursor.execute('''
                        INSERT INTO documents 
                        (filename, content, embedding, chunk_id, file_hash, metadata, 
                         header_level, section_title)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        filename, 
                        chunk_data['content'], 
                        pickle.dumps(embedding), 
                        chunk_data['chunk_id'],
                        file_hash,
                        json.dumps(chunk_metadata),
                        len(chunk_data['headers']) if chunk_data['headers'] else 0,
                        chunk_data['section_title']
                    ))
            
            conn.commit()
            conn.close()
            
            status_text.text(f"‚úÖ Added {len(chunks_data)} {chunk_type} chunks from {filename}")
            progress_bar.progress(1.0)
            return True
            
        except Exception as e:
            st.error(f"‚ùå Error adding document: {str(e)}")
            return False
    
    def search_similar(self, query: str, top_k: int = 5, use_mmr: bool = True, 
                      min_similarity: float = 0.3) -> List[Tuple[str, str, float, str]]:
        """
        Enhanced search with MMR and strict relevance filtering
        Returns: List of (filename, content, similarity, section_title)
        """
        try:
            query_embedding = self.get_embedding(query)
            if not query_embedding:
                return []
            
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute("""
                SELECT filename, content, embedding, section_title 
                FROM documents 
                ORDER BY created_at DESC
            """)
            
            documents = cursor.fetchall()
            conn.close()
            
            if not documents:
                return []
            
            # Calculate similarities and filter by minimum threshold
            results = []
            for filename, content, embedding_blob, section_title in documents:
                stored_embedding = pickle.loads(embedding_blob)
                similarity = cosine_similarity(
                    [query_embedding], 
                    [stored_embedding]
                )[0][0]
                
                # Only include results above minimum similarity threshold
                if similarity >= min_similarity:
                    results.append((filename, content, embedding_blob, section_title, similarity))
            
            if not results:
                return []  # No results above threshold
            
            # Sort by similarity for MMR or regular search
            if use_mmr and len(results) > 1:
                # Prepare documents for MMR (filename, content, embedding_blob, section_title)
                mmr_docs = [(r[0], r[1], r[2], r[3]) for r in results]
                selected_docs = mmr_search(query_embedding, mmr_docs, top_k)
                
                # Convert back to expected format and recalculate similarity for display
                final_results = []
                for filename, content, embedding_blob, section_title in selected_docs:
                    stored_embedding = pickle.loads(embedding_blob)
                    similarity = cosine_similarity([query_embedding], [stored_embedding])[0][0]
                    final_results.append((filename, content, similarity, section_title))
                
                return final_results
            else:
                # Regular similarity search
                results.sort(key=lambda x: x[4], reverse=True)
                return [(r[0], r[1], r[4], r[3]) for r in results[:top_k]]
            
        except Exception as e:
            st.error(f"‚ùå Error searching: {str(e)}")
            return []
    
    def get_stats(self) -> Dict:
        """Get knowledge base statistics"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("SELECT COUNT(DISTINCT filename) FROM documents")
            total_docs = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM documents")
            total_chunks = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM chat_sessions")
            total_chats = cursor.fetchone()[0]
            
            # Get chunk type distribution
            cursor.execute("""
                SELECT 
                    SUM(CASE WHEN metadata LIKE '%markdown%' THEN 1 ELSE 0 END) as markdown_chunks,
                    SUM(CASE WHEN metadata NOT LIKE '%markdown%' THEN 1 ELSE 0 END) as text_chunks
                FROM documents
            """)
            chunk_stats = cursor.fetchone()
            
            conn.close()
            
            return {
                "total_documents": total_docs,
                "total_chunks": total_chunks,
                "total_chat_sessions": total_chats,
                "markdown_chunks": chunk_stats[0] or 0,
                "text_chunks": chunk_stats[1] or 0
            }
        except Exception as e:
            return {"error": str(e)}
    
    def reset_knowledge_base(self) -> bool:
        """Reset entire knowledge base"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("DELETE FROM documents")
            cursor.execute("DELETE FROM chat_sessions")
            cursor.execute("VACUUM")  # Reclaim space
            
            conn.commit()
            conn.close()
            return True
        except Exception as e:
            st.error(f"‚ùå Error resetting knowledge base: {str(e)}")
            return False
    
    def reindex_documents(self) -> bool:
        """Re-create embeddings for all documents"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Get all documents
            cursor.execute("SELECT id, content FROM documents")
            documents = cursor.fetchall()
            
            if not documents:
                conn.close()
                st.info("‚ÑπÔ∏è No documents to reindex")
                return True
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            success_count = 0
            for i, (doc_id, content) in enumerate(documents):
                status_text.text(f"Re-indexing document {i+1}/{len(documents)}...")
                progress_bar.progress((i + 1) / len(documents))
                
                # Get new embedding
                embedding = self.get_embedding(content)
                if embedding:
                    # Update embedding in database
                    cursor.execute(
                        "UPDATE documents SET embedding = ? WHERE id = ?",
                        (pickle.dumps(embedding), doc_id)
                    )
                    success_count += 1
            
            conn.commit()
            conn.close()
            
            status_text.text(f"‚úÖ Re-indexed {success_count}/{len(documents)} documents")
            progress_bar.progress(1.0)
            return True
            
        except Exception as e:
            st.error(f"‚ùå Error re-indexing: {str(e)}")
            return False

def generate_rag_response(query: str, context_docs: List[Tuple[str, str, float, str]], 
                         model: str = "qwen2.5:14b", strict_mode: bool = True) -> Optional[str]:
    """Generate response using RAG context with enhanced source attribution"""
    try:
        if not context_docs:
            if strict_mode:
                return "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö"
            else:
                return None
        
        # Prepare context with source attribution
        context_parts = []
        for i, (filename, content, similarity, section_title) in enumerate(context_docs[:3]):
            section_info = f" (‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: {section_title})" if section_title else ""
            context_parts.append(
                f"[‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà {i+1}: {filename}{section_info} - ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: {similarity:.3f}]\n{content}"
            )
        
        context_text = "\n\n---\n\n".join(context_parts)
        
        # Create enhanced prompt
        prompt = f"""‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI Assistant ‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô

‡∏Å‡∏é‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:
1. ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
2. ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î"
3. ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡πÇ‡∏î‡∏¢‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
4. ‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏ï‡πà‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≤‡∏î‡πÄ‡∏î‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£

‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£:
{context_text}

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {query}

‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö (‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á):"""
        
        # Call API
        response = requests.post(
            OLLAMA_API_URL,
            json={
                "model": model,
                "prompt": prompt,
                "temperature": 0.2,  # Lower temperature for more factual responses
                "top_p": 0.8,
                "stream": False
            },
            timeout=120
        )
        response.raise_for_status()
        
        result = response.json()
        answer = result.get('response', '')
        
        if answer:
            # Add source references at the end
            source_refs = "\n\nüìö **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:**\n"
            for i, (filename, content, similarity, section_title) in enumerate(context_docs[:3]):
                section_info = f" ‚Üí {section_title}" if section_title else ""
                source_refs += f"- **‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà {i+1}:** {filename}{section_info} (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: {similarity:.1%})\n"
            
            return answer + source_refs
        
        return None
        
    except Exception as e:
        st.error(f"‚ùå Error generating RAG response: {str(e)}")
        return None

# ==================== ORIGINAL OCR FUNCTIONS (unchanged) ====================

def check_system_dependencies():
    """Check if system dependencies are available"""
    dependencies = {
        'poppler': False,
        'pdf2image': PDF2IMAGE_AVAILABLE,
        'pymupdf_available': False
    }
    
    # Check for poppler-utils
    try:
        result = subprocess.run(['pdftoppm', '--help'], 
                              capture_output=True, text=True, timeout=5)
        dependencies['poppler'] = result.returncode == 0
    except (subprocess.TimeoutExpired, FileNotFoundError, subprocess.SubprocessError):
        dependencies['poppler'] = False
    
    # Check for PyMuPDF as alternative
    try:
        import fitz  # PyMuPDF
        dependencies['pymupdf_available'] = True
    except ImportError:
        dependencies['pymupdf_available'] = False
    
    return dependencies

def convert_pdf_to_images_pypdf2(pdf_file, quality="medium") -> List[Image.Image]:
    """Fallback: Convert PDF to images using PyPDF2 (text extraction only)"""
    try:
        pdf_file.seek(0)
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        images = []
        
        # Create a simple text image for each page
        for page_num, page in enumerate(pdf_reader.pages):
            try:
                text = page.extract_text()
                if text.strip():
                    # Create a simple white image with text information
                    img = Image.new('RGB', (800, 1000), color='white')
                    images.append(img)
                else:
                    # Empty page
                    img = Image.new('RGB', (800, 1000), color='white')
                    images.append(img)
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Error processing page {page_num + 1}: {str(e)}")
                # Create placeholder image
                img = Image.new('RGB', (800, 1000), color='white')
                images.append(img)
        
        pdf_doc.close()
        return images
        
    except ImportError:
        st.error("‚ùå PyMuPDF (fitz) not available")
        return []
    except Exception as e:
        st.error(f"‚ùå Error using PyMuPDF: {str(e)}")
        return []

def convert_pdf_to_images(pdf_file, quality="medium") -> List[Image.Image]:
    """Convert PDF to images with multiple fallback methods"""
    
    # Method 1: Try pdf2image with poppler
    if PDF2IMAGE_AVAILABLE:
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                pdf_file.seek(0)
                tmp_file.write(pdf_file.read())
                tmp_file.flush()
                
                # Set DPI based on quality
                dpi = {"high": 300, "medium": 200, "low": 150}.get(quality, 200)
                
                # Try to convert
                images = pdf2image.convert_from_path(tmp_file.name, dpi=dpi)
                
                # Clean up
                os.unlink(tmp_file.name)
                
                if images:
                    st.success(f"‚úÖ PDF converted successfully using pdf2image ({len(images)} pages)")
                    return images
                    
        except Exception as e:
            st.warning(f"‚ö†Ô∏è pdf2image failed: {str(e)}")
            # Clean up temp file if it exists
            try:
                if 'tmp_file' in locals():
                    os.unlink(tmp_file.name)
            except:
                pass
    
    # Method 2: Try PyMuPDF
    st.info("üîÑ Trying alternative PDF converter (PyMuPDF)...")
    images = convert_pdf_with_pymupdf(pdf_file, quality)
    if images:
        st.success(f"‚úÖ PDF converted using PyMuPDF ({len(images)} pages)")
        return images
    
    # Method 3: Fallback to PyPDF2 (text extraction)
    st.warning("‚ö†Ô∏è Using fallback method (PyPDF2) - limited functionality")
    images = convert_pdf_to_images_pypdf2(pdf_file, quality)
    if images:
        st.info(f"‚ÑπÔ∏è PDF processed using PyPDF2 fallback ({len(images)} pages)")
        return images
    
    # If all methods fail
    st.error("‚ùå All PDF conversion methods failed!")
    return []

def extract_text_from_pdf_page(pdf_file, page_num: int) -> str:
    """Extract text from specific PDF page using PyPDF2"""
    try:
        pdf_file.seek(0)
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        
        if page_num < len(pdf_reader.pages):
            page = pdf_reader.pages[page_num]
            return page.extract_text()
        return ""
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Text extraction failed for page {page_num + 1}: {str(e)}")
        return ""

def image_to_base64(image: Image.Image) -> str:
    """Convert PIL image to base64 string"""
    try:
        buffer = BytesIO()
        # Optimize image size
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Resize if too large (max 2048 pixels)
        max_size = 2048
        if max(image.size) > max_size:
            ratio = max_size / max(image.size)
            new_size = tuple(int(dim * ratio) for dim in image.size)
            image = image.resize(new_size, Image.Resampling.LANCZOS)
        
        image.save(buffer, format='PNG', optimize=True)
        return base64.b64encode(buffer.getvalue()).decode()
    except Exception as e:
        st.error(f"‚ùå Error converting image to base64: {str(e)}")
        return ""

def get_ocr_prompt(prompt_type: str, raw_text: str = "") -> str:
    """Get OCR prompt based on type"""
    if prompt_type == "structure":
        return f"""Below is an image of a document page, along with its dimensions and possibly some raw textual content previously extracted from it. 
Note that the text extraction may be incomplete or partially missing. Carefully consider both the layout and any available text to reconstruct the document accurately.
Your task is to return the markdown representation of this document, presenting tables in HTML format as they naturally appear.
If the document contains images or figures, analyze them and include the tag <figure>IMAGE_ANALYSIS</figure> in the appropriate location.
Your final output must be in JSON format with a single key `natural_text` containing the response.
RAW_TEXT_START
{raw_text}
RAW_TEXT_END"""
    else:
        return f"""Below is an image of a document page along with its dimensions. 
Simply return the markdown representation of this document, presenting tables in markdown format as they naturally appear.
If the document contains images, use a placeholder like dummy.png for each image.
Your final output must be in JSON format with a single key `natural_text` containing the response.
RAW_TEXT_START
{raw_text}
RAW_TEXT_END"""

def call_ollama_api(model: str, prompt: str, image_base64: str, params: dict) -> Optional[str]:
    """Call Ollama API for OCR processing with error handling"""
    try:
        # Validate inputs
        if not image_base64:
            raise ValueError("Empty image data")
        
        # For OCR models, we need to use chat completion format
        if "typhoon-ocr" in model:
            payload = {
                "model": model,
                "prompt": prompt,
                "images": [image_base64],
                "temperature": params.get('temperature', 0.1),
                "top_p": params.get('top_p', 0.6),
                "num_predict": params.get('max_tokens', 12000),
                "stream": False
            }
        else:
            # For other models, use standard format
            payload = {
                "model": model,
                "prompt": f"Analyze this image and extract all text content. Return the result in markdown format.\n\nImage: data:image/png;base64,{image_base64}\n\n{prompt}",
                "temperature": params.get('temperature', 0.1),
                "top_p": params.get('top_p', 0.6),
                "num_predict": params.get('max_tokens', 12000),
                "stream": False
            }
        
        # Add timeout and retry logic
        max_retries = 2
        for attempt in range(max_retries):
            try:
                response = requests.post(
                    OLLAMA_API_URL, 
                    json=payload, 
                    timeout=300,  # 5 minutes timeout
                    headers={'Content-Type': 'application/json'}
                )
                response.raise_for_status()
                
                result = response.json()
                content = result.get('response', '')
                
                if content.strip():
                    return content
                else:
                    raise ValueError("Empty response from API")
                    
            except requests.exceptions.Timeout:
                if attempt < max_retries - 1:
                    st.warning(f"‚ö†Ô∏è Timeout on attempt {attempt + 1}, retrying...")
                    continue
                else:
                    raise
            except requests.exceptions.ConnectionError:
                if attempt < max_retries - 1:
                    st.warning(f"‚ö†Ô∏è Connection error on attempt {attempt + 1}, retrying...")
                    continue
                else:
                    raise
        
    except requests.exceptions.RequestException as e:
        error_msg = f"API Request Error: {str(e)}"
        if hasattr(e, 'response') and e.response is not None:
            try:
                error_detail = e.response.json()
                error_msg += f" - {error_detail}"
            except:
                error_msg += f" - HTTP {e.response.status_code}"
        st.error(f"‚ùå {error_msg}")
        return None
    except ValueError as e:
        st.error(f"‚ùå Validation Error: {str(e)}")
        return None
    except Exception as e:
        st.error(f"‚ùå Processing Error: {str(e)}")
        st.error(f"Debug: {traceback.format_exc()}")
        return None

def process_single_document(file, model: str, params: dict) -> Dict[str, Any]:
    """Process a single document with comprehensive error handling"""
    results = {
        'filename': file.name,
        'file_type': file.type,
        'file_size': file.size,
        'pages': [],
        'total_pages': 0,
        'success': False,
        'error': None,
        'warnings': []
    }
    
    try:
        # Validate file size (max 10MB)
        max_size = 10 * 1024 * 1024  # 10MB
        if file.size > max_size:
            results['error'] = f"File too large ({file.size/1024/1024:.1f}MB). Maximum size is 10MB."
            return results
        
        # Handle different file types
        if file.type == 'application/pdf':
            st.info(f"üìÑ Processing PDF: {file.name}")
            images = convert_pdf_to_images(file, params.get('image_quality', 'medium'))
            results['total_pages'] = len(images) if images else 0
            
            if not images:
                results['error'] = "Could not convert PDF to images"
                return results
                
        elif file.type.startswith('image/'):
            st.info(f"üñºÔ∏è Processing Image: {file.name}")
            try:
                image = Image.open(file)
                images = [image]
                results['total_pages'] = 1
            except Exception as e:
                results['error'] = f"Could not open image file: {str(e)}"
                return results
        else:
            results['error'] = f"Unsupported file type: {file.type}"
            return results
        
        # Process each page/image
        for i, image in enumerate(images):
            page_result = {
                'page_number': i + 1,
                'success': False,
                'content': '',
                'raw_response': '',
                'extracted_text': '',
                'processing_time': 0,
                'error': None
            }
            
            try:
                import time
                start_time = time.time()
                
                # For PDF pages, try to extract text first
                if file.type == 'application/pdf':
                    try:
                        page_result['extracted_text'] = extract_text_from_pdf_page(file, i)
                    except:
                        pass
                
                # Convert image to base64
                image_base64 = image_to_base64(image)
                if not image_base64:
                    page_result['error'] = "Failed to convert image to base64"
                    continue
                
                # Get appropriate prompt
                prompt = get_ocr_prompt(
                    params.get('prompt_type', 'default'),
                    page_result['extracted_text']
                )
                
                # Show processing status
                with st.spinner(f"üîÑ Processing page {i + 1}..."):
                    # Call API
                    response = call_ollama_api(model, prompt, image_base64, params)
                
                if response:
                    page_result['raw_response'] = response
                    page_result['processing_time'] = time.time() - start_time
                    
                    # Try to parse JSON response
                    try:
                        json_response = json.loads(response)
                        page_result['content'] = json_response.get('natural_text', response)
                    except json.JSONDecodeError:
                        page_result['content'] = response
                    
                    # Validate content
                    if page_result['content'].strip():
                        page_result['success'] = True
                        st.success(f"‚úÖ Page {i + 1} processed successfully ({page_result['processing_time']:.1f}s)")
                    else:
                        page_result['error'] = "Empty content returned"
                        st.warning(f"‚ö†Ô∏è Page {i + 1} returned empty content")
                else:
                    page_result['error'] = "No response from API"
                    st.error(f"‚ùå Page {i + 1} failed - no response")
                
            except Exception as e:
                page_result['error'] = str(e)
                page_result['processing_time'] = time.time() - start_time if 'start_time' in locals() else 0
                st.error(f"‚ùå Page {i + 1} failed: {str(e)}")
            
            results['pages'].append(page_result)
        
        # Check if any pages were successful
        successful_pages = [p for p in results['pages'] if p['success']]
        results['success'] = len(successful_pages) > 0
        
        if results['success']:
            results['summary'] = {
                'successful_pages': len(successful_pages),
                'failed_pages': len(results['pages']) - len(successful_pages),
                'total_processing_time': sum(p['processing_time'] for p in results['pages']),
                'average_time_per_page': sum(p['processing_time'] for p in results['pages']) / len(results['pages']) if results['pages'] else 0
            }
        
    except Exception as e:
        results['error'] = f"Document processing failed: {str(e)}"
        st.error(f"‚ùå Fatal error processing {file.name}: {str(e)}")
    
    return results

def process_documents(uploaded_files, model: str, params: dict):
    """Process uploaded documents with enhanced error handling and progress tracking"""
    
    # Handle single file vs multiple files
    files_to_process = uploaded_files if isinstance(uploaded_files, list) else [uploaded_files]
    
    st.header("üîÑ Processing Results")
    
    # Show system status first
    deps = check_system_dependencies()
    
    with st.expander("üîß System Status", expanded=False):
        st.write("**Dependencies Status:**")
        status_items = [
            ("Poppler (PDF2Image)", "‚úÖ" if deps['poppler'] else "‚ùå"),
            ("pdf2image Library", "‚úÖ" if deps['pdf2image'] else "‚ùå"),
            ("PyMuPDF Alternative", "‚úÖ" if deps['pymupdf_available'] else "‚ùå"),
            ("PyPDF2 Fallback", "‚úÖ")  # Always available
        ]
        
        for item, status in status_items:
            st.write(f"{status} {item}")
        
        if not deps['poppler'] and not deps['pymupdf_available']:
            st.warning("‚ö†Ô∏è Limited PDF processing capabilities. Consider installing poppler or PyMuPDF.")
    
    # Progress tracking
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    all_results = []
    total_files = len(files_to_process)
    
    for i, file in enumerate(files_to_process):
        status_text.text(f"üìÑ Processing {file.name} ({i+1}/{total_files})...")
        progress_bar.progress(i / total_files)
        
        # Reset file pointer
        file.seek(0)
        
        # Process document
        result = process_single_document(file, model, params)
        all_results.append(result)
        
        # Display results for this file
        status_emoji = "‚úÖ" if result['success'] else "‚ùå"
        summary_text = ""
        
        if result['success'] and 'summary' in result:
            s = result['summary']
            summary_text = f" - {s['successful_pages']}/{result['total_pages']} pages, {s['total_processing_time']:.1f}s total"
        
        with st.expander(f"{status_emoji} {file.name}{summary_text}", expanded=result['success']):
            if result['success']:
                # Show summary
                if 'summary' in result:
                    s = result['summary']
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("‚úÖ Success", f"{s['successful_pages']}/{result['total_pages']}")
                    with col2:
                        st.metric("‚è±Ô∏è Total Time", f"{s['total_processing_time']:.1f}s")
                    with col3:
                        st.metric("üìä Avg/Page", f"{s['average_time_per_page']:.1f}s")
                    with col4:
                        st.metric("üìÑ File Size", f"{result['file_size']/1024:.1f}KB")
                
                # RAG Integration - Auto-add to knowledge base option
                if st.button(f"üß† Add to Knowledge Base", key=f"add_kb_{i}"):
                    # Combine all successful page content
                    combined_content = "\n\n".join([
                        page['content'] for page in result['pages'] if page['success']
                    ])
                    
                    # Add to RAG knowledge base
                    kb = RAGKnowledgeBase(st.session_state.rag_db_path)
                    success = kb.add_document(
                        filename=file.name,
                        content=combined_content,
                        metadata={
                            "processing_time": s.get('total_processing_time', 0),
                            "model_used": model,
                            "successful_pages": s.get('successful_pages', 0)
                        }
                    )
                    if success:
                        st.success(f"‚úÖ Added {file.name} to knowledge base!")
                        st.rerun()
                
                # Show results for each page
                for page in result['pages']:
                    if page['success']:
                        st.subheader(f"üìÑ Page {page['page_number']}")
                        
                        # Tabs for different views
                        tab1, tab2, tab3, tab4 = st.tabs(["üìñ Preview", "üìù Markdown", "üîß Raw", "‚ÑπÔ∏è Info"])
                        
                        with tab1:
                            if params.get('output_format') == 'html':
                                st.markdown(page['content'], unsafe_allow_html=True)
                            else:
                                st.markdown(page['content'])
                        
                        with tab2:
                            st.code(page['content'], language='markdown')
                        
                        with tab3:
                            st.code(page['raw_response'], language='json')
                        
                        with tab4:
                            st.write(f"**Processing Time:** {page['processing_time']:.1f}s")
                            st.write(f"**Content Length:** {len(page['content'])} characters")
                            if page['extracted_text']:
                                st.write(f"**Extracted Text Length:** {len(page['extracted_text'])} characters")
                            
                        # Download button
                        file_extension = {
                            'markdown': 'md',
                            'html': 'html', 
                            'json': 'json'
                        }.get(params.get('output_format', 'markdown'), 'md')
                        
                        st.download_button(
                            f"üíæ Download Page {page['page_number']}",
                            page['content'],
                            f"{file.name}_page_{page['page_number']}.{file_extension}",
                            f"text/{file_extension}"
                        )
                    else:
                        st.error(f"‚ùå Page {page['page_number']} failed: {page.get('error', 'Unknown error')}")
                        if page.get('processing_time', 0) > 0:
                            st.write(f"Processing time: {page['processing_time']:.1f}s")
            else:
                st.error(f"‚ùå Failed to process {file.name}")
                if result.get('error'):
                    st.write(f"**Error:** {result['error']}")
                
                # Show system recommendations
                if 'pdf' in file.type.lower() and not deps['poppler'] and not deps['pymupdf_available']:
                    st.info("""
                    **üí° Recommendation for PDF processing:**
                    
                    For better PDF support, install one of these:
                    
                    **Option 1: Install Poppler (Recommended)**
                    ```bash
                    # Ubuntu/Debian
                    apt-get install poppler-utils
                    
                    # CentOS/RHEL
                    yum install poppler-utils
                    
                    # macOS
                    brew install poppler
                    ```
                    
                    **Option 2: Install PyMuPDF**
                    ```bash
                    pip install PyMuPDF
                    ```
                    """)
    
    # Final progress
    progress_bar.progress(1.0)
    status_text.text("üéâ Processing complete!")
    
    # Summary statistics
    successful_files = sum(1 for r in all_results if r['success'])
    total_pages = sum(r['total_pages'] for r in all_results)
    successful_pages = sum(len([p for p in r['pages'] if p['success']]) for r in all_results)
    total_processing_time = sum(
        r.get('summary', {}).get('total_processing_time', 0) for r in all_results if r['success']
    )
    
    # Display final summary
    st.markdown("---")
    st.subheader("üìä Processing Summary")
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("üìÅ Files", f"{successful_files}/{len(files_to_process)}")
    with col2:
        st.metric("üìÑ Pages", f"{successful_pages}/{total_pages}")
    with col3:
        st.metric("‚è±Ô∏è Total Time", f"{total_processing_time:.1f}s")
    with col4:
        st.metric("ü§ñ Model", AVAILABLE_MODELS[model]['name'])
    
    # Show processing efficiency
    if successful_pages > 0:
        avg_time_per_page = total_processing_time / successful_pages
        pages_per_minute = 60 / avg_time_per_page if avg_time_per_page > 0 else 0
        
        st.info(f"""
        **‚ö° Performance Metrics:**
        - Average processing time: **{avg_time_per_page:.1f} seconds per page**
        - Processing rate: **{pages_per_minute:.1f} pages per minute**
        - Success rate: **{(successful_pages/total_pages)*100:.1f}%**
        """)
    
    # Bulk download for successful results
    if successful_pages > 0:
        # Combine all successful content
        combined_content = f"# Typhoon OCR Results\n\n"
        combined_content += f"**Processing Summary:**\n"
        combined_content += f"- Files processed: {successful_files}/{len(files_to_process)}\n"
        combined_content += f"- Pages processed: {successful_pages}/{total_pages}\n"
        combined_content += f"- Model used: {AVAILABLE_MODELS[model]['name']}\n"
        combined_content += f"- Total processing time: {total_processing_time:.1f}s\n\n"
        combined_content += f"---\n\n"
        
        for result in all_results:
            if result['success']:
                combined_content += f"# üìÑ {result['filename']}\n\n"
                if 'summary' in result:
                    s = result['summary']
                    combined_content += f"**File Summary:**\n"
                    combined_content += f"- Pages: {s['successful_pages']}/{result['total_pages']}\n"
                    combined_content += f"- Processing time: {s['total_processing_time']:.1f}s\n\n"
                
                for page in result['pages']:
                    if page['success']:
                        combined_content += f"## üöÄ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: OCR Processing (‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£)
1. **‡πÄ‡∏õ‡∏¥‡∏î‡πÅ‡∏ó‡πá‡∏ö "üìÅ Upload & OCR"**
2. **‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå** (PDF, PNG, JPG, JPEG - ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 10MB)
3. **‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå:**
   - **Model:** Typhoon OCR 7B ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OCR
   - **Temperature:** 0.1 (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î)
   - **Prompt Type:** Structure ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
4. **‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° "üöÄ Process Document(s)"**
5. **‡∏£‡∏≠‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå** ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤ Enhanced Knowledge Base
‡∏°‡∏µ **3 ‡∏ß‡∏¥‡∏ò‡∏µ‡∏´‡∏•‡∏±‡∏Å** ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏ö Smart Chunking:

#### üîπ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: Auto-add ‡∏à‡∏≤‡∏Å‡∏ú‡∏• OCR
- ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å OCR ‡πÄ‡∏™‡∏£‡πá‡∏à ‡∏à‡∏∞‡∏°‡∏µ‡∏õ‡∏∏‡πà‡∏° **"üß† Add to Knowledge Base"**
- ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡πÉ‡∏ä‡πâ Regular Text Chunking ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå OCR

#### üîπ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: Upload Markdown Files (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥!)
- ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πá‡∏ö **"üß† Knowledge Base"**
- ‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå .md ‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô **"Method 1"**
- ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡πÉ‡∏ä‡πâ **Header-Aware Chunking** ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
- ‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏ö‡∏ó

#### üîπ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 3: Direct Text Input
- ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å **"Content Format: markdown"** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Smart Chunking
- ‡πÉ‡∏™‡πà‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Markdown ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠
- ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡πÅ‡∏¢‡∏Å chunk ‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏ñ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ Enhanced AI Chat
1. **‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πá‡∏ö "üí¨ AI Chat"**
2. **‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Chat Settings:**
   - **Model:** Qwen2.5 14B ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‡∏´‡∏£‡∏∑‡∏≠ Typhoon2 8B ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
   - **MMR Search:** ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢
   - **Strict Mode:** ‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£ hallucination
3. **‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á Advanced Settings:**
   - **Top K:** 5-7 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
   - **Min Similarity:** 0.3 (‡∏õ‡∏£‡∏±‡∏ö‡∏•‡∏î‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)
   - **Diversity Weight:** 0.3 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏î‡∏∏‡∏•
4. **‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°** ‡πÅ‡∏•‡∏∞‡∏£‡∏≠‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á

---

## üéØ ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á

### üìä ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Model ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°

#### OCR Processing:
- **Typhoon OCR 7B:** ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç OCR ‡πÑ‡∏ó‡∏¢-‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©
- **Temperature: 0.1, Top P: 0.6** ‚Üí ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î

#### Q&A Chat:
- **Typhoon2 8B:** ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
- **Qwen2.5 14B:** ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
- **Temperature: 0.2-0.3** ‚Üí ‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå

#### Embeddings:
- **Nomic Embed Text:** ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings (‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)

### üîç ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Search Algorithm

#### MMR Search (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥):
- **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ:** ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢ ‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô
- **Top K:** 5-7 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ, 3-5 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞
- **Diversity Weight:** 0.2-0.4 (0.3 ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏µ)

#### Similarity Search:
- **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ:** ‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
- **‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠:** ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏°‡∏≤‡∏Å

#### Strict Mode:
- **‡πÄ‡∏õ‡∏¥‡∏î:** ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô AI ‡πÅ‡∏ï‡πà‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
- **‡∏õ‡∏¥‡∏î:** ‡πÉ‡∏´‡πâ AI ‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á

---

## üß† ‡∏£‡∏∞‡∏ö‡∏ö Smart RAG ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?

### üîß Header-Aware Markdown Chunking:
```markdown
# ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÉ‡∏´‡∏ç‡πà
‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1...

## ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏¢‡πà‡∏≠‡∏¢ A
‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2...

## ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏¢‡πà‡∏≠‡∏¢ B  
‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3...
```

**‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô:**
- Chunk 1: ‡∏£‡∏ß‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÉ‡∏´‡∏ç‡πà + ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1
- Chunk 2: ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÉ‡∏´‡∏ç‡πà + ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏¢‡πà‡∏≠‡∏¢ A + ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2  
- Chunk 3: ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÉ‡∏´‡∏ç‡πà + ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏¢‡πà‡∏≠‡∏¢ B + ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3

**‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå:**
- ‡πÄ‡∏Å‡πá‡∏ö‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ç‡∏≠‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å chunk
- ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
- AI ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô

### üéØ MMR Algorithm:
1. **‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á** (Relevance) ‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
2. **‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢** (Diversity) ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå  
3. **‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏°‡∏î‡∏∏‡∏•** ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢
4. **‡∏Ñ‡∏∑‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå** ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô

---

## üì± ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÅ‡∏ó‡πá‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î

### üìÅ Upload & OCR
**‡∏à‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå:** ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°

**‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô:**
1. ‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå (‡πÄ‡∏õ‡∏¥‡∏î Batch Processing ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå)
2. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Model: Typhoon OCR 7B  
3. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤:
   - **Structure mode** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ï‡∏≤‡∏£‡∏≤‡∏á
   - **High quality** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏•‡πá‡∏Å
4. ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
5. ‡∏Å‡∏î "üß† Add to Knowledge Base" ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£

### üß† Knowledge Base (‡πÅ‡∏ó‡πá‡∏ö‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç!)
**‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏´‡∏•‡∏±‡∏Å:**

#### ‡∏Å‡∏≤‡∏£‡∏î‡∏π‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥:
- **Total Documents:** ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
- **MD Chunks:** ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô chunk ‡∏à‡∏≤‡∏Å Markdown (Smart Chunking)
- **Text Chunks:** ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô chunk ‡∏à‡∏≤‡∏Å Text ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤

#### ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£:
- **üîÑ Re-index:** ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡πÉ‡∏´‡∏°‡πà (‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô embedding model)
- **üóëÔ∏è Reset KB:** ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô 3 ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô)

#### ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£:
- **Markdown Files:** ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á
- **Direct Input:** ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å "markdown format" ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Smart Chunking

#### Document Browser:
- ‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå
- ‡πÅ‡∏™‡∏î‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô sections ‡πÅ‡∏•‡∏∞ chunk type
- ‡∏•‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÑ‡∏î‡πâ

### üí¨ AI Chat (‡πÅ‡∏ó‡πá‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô)
**‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:**

#### Chat Settings:
- **Chat Model:** ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏≤‡∏° use case
- **Use MMR Search:** ‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏™‡∏°‡∏≠ (‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏û‡∏¥‡πÄ‡∏®‡∏©)
- **Strict Mode:** ‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô hallucination

#### Advanced Settings:
- **Top K (5-7):** ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
- **Min Similarity (0.3):** ‡∏Å‡∏£‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
- **Diversity Weight (0.3):** ‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢

#### ‡∏õ‡∏∏‡πà‡∏°‡∏ï‡πà‡∏≤‡∏á ‡πÜ:
- **üöÄ Ask:** ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡πÑ‡∏î‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á
- **üîç Search Only:** ‡∏î‡∏π‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡πÑ‡∏°‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö)
- **üíæ Export Chat:** ‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤
- **üóëÔ∏è Clear Chat:** ‡∏•‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤

---

## ‚ö° ‡πÄ‡∏Ñ‡∏•‡πá‡∏î‡∏•‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á

### üéØ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û OCR:
1. **‡∏™‡πÅ‡∏Å‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏π‡∏á** (300 DPI ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ)
2. **‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°** ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏á‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏™‡∏á‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô
3. **‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ï‡∏£‡∏á** ‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏µ‡∏¢‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏¥‡∏î‡πÄ‡∏ö‡∏µ‡πâ‡∏¢‡∏ß
4. **‡πÉ‡∏ä‡πâ Structure mode** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
5. **‡πÅ‡∏ö‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà** ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤

### üß† ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û Knowledge Base:

#### ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ Markdown:
```markdown
# ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏Å: ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô Q1 2024

## ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏á‡∏≤‡∏ô
‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡πÉ‡∏ô‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™‡πÅ‡∏£‡∏Å‡∏õ‡∏µ 2024 ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà 150 ‡∏•‡πâ‡∏≤‡∏ô‡∏ö‡∏≤‡∏ó...

### ‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏™‡πà‡∏ß‡∏ô‡∏á‡∏≤‡∏ô
- ‡∏™‡πà‡∏ß‡∏ô‡∏á‡∏≤‡∏ô‡∏Ç‡∏≤‡∏¢: 100 ‡∏•‡πâ‡∏≤‡∏ô‡∏ö‡∏≤‡∏ó
- ‡∏™‡πà‡∏ß‡∏ô‡∏á‡∏≤‡∏ô‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£: 50 ‡∏•‡πâ‡∏≤‡∏ô‡∏ö‡∏≤‡∏ó

## ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡πÑ‡∏£
‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡∏£‡∏ß‡∏° 120 ‡∏•‡πâ‡∏≤‡∏ô‡∏ö‡∏≤‡∏ó ‡∏Å‡∏≥‡πÑ‡∏£‡∏™‡∏∏‡∏ó‡∏ò‡∏¥ 30 ‡∏•‡πâ‡∏≤‡∏ô‡∏ö‡∏≤‡∏ó...
```

#### ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå:
- ‚úÖ **‡∏î‡∏µ:** "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô_Q1_2024.md"
- ‚úÖ **‡∏î‡∏µ:** "‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô_‡∏£‡∏∞‡∏ö‡∏ö_CRM.md"  
- ‚ùå **‡πÑ‡∏°‡πà‡∏î‡∏µ:** "document1.md", "‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà.md"

#### ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£:
- **‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô** ‡∏Ñ‡∏ß‡∏£‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
- **‡πÅ‡∏¢‡∏Å‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÉ‡∏´‡∏ç‡πà** ‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏¢‡∏Å
- **‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó** ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô

### üí¨ ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Å‡∏≤‡∏£‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:

#### ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏î‡∏µ:
- **‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á:** "‡∏°‡∏µ‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡πÉ‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏° 2024 ‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà?"
- **‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£:** "‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô Q1 ‡∏°‡∏µ‡∏Å‡∏≥‡πÑ‡∏£‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà?"
- **‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:** "‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏á‡∏≤‡∏ô Q1 ‡∏Å‡∏±‡∏ö Q2 2024"
- **‡∏™‡∏£‡∏∏‡∏õ:** "‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô‡∏Ç‡∏≠‡∏á‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡πÉ‡∏´‡∏°‡πà"

#### ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á:
- **‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ:** "‡πÄ‡∏•‡πà‡∏≤‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£"
- **‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:** "‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?"
- **‡∏Ñ‡∏•‡∏∏‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠:** "‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à‡∏ö‡πâ‡∏≤‡∏á?"

### üîß ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô:

#### OCR ‡πÑ‡∏°‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥:
- ‡πÄ‡∏û‡∏¥‡πà‡∏° Image Quality ‡πÄ‡∏õ‡πá‡∏ô "high"
- ‡πÉ‡∏ä‡πâ Structure mode ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏î‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
- ‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏†‡∏≤‡∏û‡πÅ‡∏ó‡∏ô PDF

#### ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤:
- ‡∏•‡∏î Min Similarity ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 0.2 ‡∏´‡∏£‡∏∑‡∏≠ 0.1
- ‡∏õ‡∏¥‡∏î Strict Mode ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
- ‡πÄ‡∏û‡∏¥‡πà‡∏° Top K ‡πÄ‡∏õ‡πá‡∏ô 8-10
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤ Knowledge Base ‡πÅ‡∏•‡πâ‡∏ß

#### AI ‡∏ï‡∏≠‡∏ö‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:
- ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡∏Ç‡∏∂‡πâ‡∏ô
- ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å Qwen2.5 ‡πÄ‡∏õ‡πá‡∏ô Typhoon2 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
- ‡πÉ‡∏ä‡πâ Search Only ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏à‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö

#### ‡∏£‡∏∞‡∏ö‡∏ö‡∏ä‡πâ‡∏≤:
- ‡∏•‡∏î Top K ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 3-5
- ‡πÉ‡∏ä‡πâ Similarity Search ‡πÅ‡∏ó‡∏ô MMR ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î (‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 10MB)

---

## üöÄ ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡πÅ‡∏•‡∏∞ Best Practices

### üìä Workflow ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£:

#### Week 1: Setup Phase
1. **‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç** ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Ç‡∏≠‡∏á‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£
2. **‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Markdown** ‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
3. **‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∏‡∏î** ‡∏ú‡πà‡∏≤‡∏ô Knowledge Base
4. **‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤** ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á

#### Ongoing: Maintenance Phase
1. **‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏´‡∏°‡πà** ‡∏ó‡∏∏‡∏Å‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå
2. **‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î** ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏•‡πâ‡∏≤‡∏™‡∏°‡∏±‡∏¢
3. **Re-index** ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÉ‡∏´‡∏ç‡πà
4. **Export chat history** ‡πÄ‡∏õ‡πá‡∏ô knowledge repository

### üéõÔ∏è ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏≤‡∏° Use Case:

#### ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå:
- **Model:** Qwen2.5 14B
- **MMR:** ‡πÄ‡∏õ‡∏¥‡∏î, Diversity Weight 0.4
- **Top K:** 7-10
- **Min Similarity:** 0.2

#### ‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞:
- **Model:** Typhoon2 8B (‡πÑ‡∏ó‡∏¢) ‡∏´‡∏£‡∏∑‡∏≠ Qwen2.5 (‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©)  
- **MMR:** ‡∏õ‡∏¥‡∏î (‡πÉ‡∏ä‡πâ Similarity)
- **Top K:** 3-5
- **Min Similarity:** 0.4

#### ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:
- **Model:** Qwen2.5 14B
- **MMR:** ‡πÄ‡∏õ‡∏¥‡∏î, Diversity Weight 0.3
- **Top K:** 5-7  
- **Strict Mode:** ‡∏õ‡∏¥‡∏î (‡πÉ‡∏´‡πâ flexibility)

### üîÑ Data Management Best Practices:

#### ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°:
```
‚îú‚îÄ‚îÄ ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô/
‚îÇ   ‚îú‚îÄ‚îÄ ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô_Q1_2024.md
‚îÇ   ‚îú‚îÄ‚îÄ ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô_Q2_2024.md
‚îÇ   ‚îî‚îÄ‚îÄ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°_2024.md
‚îú‚îÄ‚îÄ ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£/
‚îÇ   ‚îú‚îÄ‚îÄ ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢_HR_2024.md
‚îÇ   ‚îú‚îÄ‚îÄ ‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ã‡∏∑‡πâ‡∏≠.md
‚îÇ   ‚îî‚îÄ‚îÄ ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô_Remote.md
‚îî‚îÄ‚îÄ ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ/
    ‚îú‚îÄ‚îÄ ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠_‡∏£‡∏∞‡∏ö‡∏ö_CRM.md
    ‚îú‚îÄ‚îÄ ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ_Analytics_Platform.md
    ‚îî‚îÄ‚îÄ Troubleshooting_Guide.md
```

#### ‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏£‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏π‡πâ‡∏Ñ‡∏∑‡∏ô:
1. **Export chat history** ‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡∏à‡∏≥
2. **‡∏™‡∏≥‡∏£‡∏≠‡∏á knowledge base** ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£ export ‡πÄ‡∏õ‡πá‡∏ô Markdown
3. **‡∏ó‡∏î‡∏™‡∏≠‡∏ö re-indexing** ‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á
4. **‡πÄ‡∏Å‡πá‡∏ö backup** ‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö

---

## ‚ùì FAQ ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤

**Q: ‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏∞‡∏´‡∏≤‡∏¢‡πÑ‡∏´‡∏°‡∏ñ‡πâ‡∏≤‡∏õ‡∏¥‡∏î‡πÄ‡∏ö‡∏£‡∏≤‡∏ß‡πå‡πÄ‡∏ã‡∏≠‡∏£‡πå?**  
A: ‡πÑ‡∏°‡πà‡∏´‡∏≤‡∏¢ ‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏ä‡πâ SQLite ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏ñ‡∏≤‡∏ß‡∏£‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á

**Q: ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏≠‡∏≠‡∏ü‡πÑ‡∏•‡∏ô‡πå‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°?**  
A: ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á AI models

**Q: ‡∏ó‡∏≥‡πÑ‡∏° MMR ‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤ Similarity Search?**  
A: MMR ‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì diversity ‡∏ã‡∏∂‡πà‡∏á‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ ‡πÅ‡∏ï‡πà‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Å‡∏ß‡πà‡∏≤

**Q: Re-indexing ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£ ‡πÅ‡∏•‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ?**  
A: ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡πÉ‡∏´‡∏°‡πà ‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠:
- ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô embedding model
- ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å
- ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥

**Q: Strict Mode ‡∏Å‡∏±‡∏ö Flexible Mode ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?**  
A: 
- **Strict:** ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£, ‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
- **Flexible:** AI ‡∏≠‡∏≤‡∏à‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

**Q: ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà?**  
A: ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß 10MB, ‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå ‡πÅ‡∏ï‡πà‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏à‡∏∞‡∏•‡∏î‡∏•‡∏á‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ

**Q: ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Knowledge Base ‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°?**  
A: ‡πÑ‡∏î‡πâ ‡∏ú‡πà‡∏≤‡∏ô Document Browser ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ Reset KB ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏•‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î

**Q: ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?**  
A: 
- **‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏•‡πá‡∏Å (<50 ‡∏´‡∏ô‡πâ‡∏≤):** ‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
- **‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà:** ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏´‡∏•‡∏±‡∏Å
- **‡πÉ‡∏ä‡πâ Markdown** ‡πÄ‡∏™‡∏°‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏à‡∏≤‡∏Å Smart Chunking

---

## üéØ ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏°‡∏∑‡∏≠‡πÉ‡∏´‡∏°‡πà

### ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô 5 ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô:
1. **‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£** ‚Üí ‡πÅ‡∏ó‡πá‡∏ö Upload & OCR
2. **Add to Knowledge Base** ‚Üí ‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏°‡∏´‡∏•‡∏±‡∏á OCR ‡πÄ‡∏™‡∏£‡πá‡∏à
3. **‡πÑ‡∏õ‡∏ó‡∏µ‡πà AI Chat** ‚Üí ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô MMR + Strict Mode
4. **‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏á‡πà‡∏≤‡∏¢ ‡πÜ** ‚Üí ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö
5. **‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå** ‚Üí ‡πÄ‡∏û‡∏¥‡πà‡∏°/‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ñ‡∏£‡πà‡∏á‡∏Ñ‡∏£‡∏±‡∏î

### ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á:
1. **‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ü‡∏•‡πå Markdown** ‚Üí ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
2. **‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡∏ú‡πà‡∏≤‡∏ô Knowledge Base** ‚Üí ‡πÑ‡∏î‡πâ Smart Chunking
3. **‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á Search Parameters** ‚Üí ‡∏ï‡∏≤‡∏° use case
4. **‡πÉ‡∏ä‡πâ Search Only** ‚Üí ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏ö
5. **Export Chat History** ‚Üí ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô knowledge base

‡∏£‡∏∞‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:
- ‚úÖ **‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£**
- ‚úÖ **‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤**  
- ‚úÖ **‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£**
- ‚úÖ **‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á knowledge base ‡∏Ç‡∏≠‡∏á‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£**

        """)

# Run the app
if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        st.error(f"‚ùå Application Error: {str(e)}")
        st.error("Please refresh the page and try again.")
        st.error(f"Debug info: {traceback.format_exc()}")Page {page['page_number']}\n\n"
                        combined_content += f"{page['content']}\n\n"
                        combined_content += f"---\n\n"
        
        # File extension based on output format
        file_extension = {
            'markdown': 'md',
            'html': 'html', 
            'json': 'json'
        }.get(params.get('output_format', 'markdown'), 'md')
        
        col1, col2 = st.columns([1, 1])
        with col1:
            st.download_button(
                "üì¶ Download All Results",
                combined_content,
                f"typhoon_ocr_results.{file_extension}",
                f"text/{file_extension}",
                help=f"Download combined results in {params.get('output_format', 'markdown')} format"
            )
        
        with col2:
            if st.button("üß† Add All to Knowledge Base"):
                kb = RAGKnowledgeBase(st.session_state.rag_db_path)
                added_count = 0
                for result in all_results:
                    if result['success']:
                        combined_content = "\n\n".join([
                            page['content'] for page in result['pages'] if page['success']
                        ])
                        if kb.add_document(
                            filename=result['filename'],
                            content=combined_content,
                            metadata={"batch_processed": True}
                        ):
                            added_count += 1
                
                if added_count > 0:
                    st.success(f"‚úÖ Added {added_count} documents to knowledge base!")
                    st.rerun()

def main():
    # Header
    st.markdown("""
    <div class="main-header">
        <h1>üå™Ô∏è Typhoon OCR with Enhanced RAG</h1>
        <p>AI-Powered Thai-English Document Parser + Smart Knowledge Q&A</p>
        <p>Powered by SCB 10X with AI NT North Team</p>
    </div>
    """, unsafe_allow_html=True)

    # System status check
    deps = check_system_dependencies()
    
    # Show system warnings if needed
    if not deps['poppler'] and not deps['pymupdf_available']:
        st.markdown("""
        <div class="warning-message">
            <strong>‚ö†Ô∏è Limited PDF Processing Capability</strong><br>
            For optimal PDF processing, please install poppler-utils or PyMuPDF.<br>
            Currently using PyPDF2 fallback which has limited functionality.
        </div>
        """, unsafe_allow_html=True)

    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è Settings")
        
        # System status in sidebar
        with st.expander("üîß System Status"):
            st.write("**PDF Processing:**")
            if deps['poppler'] and deps['pdf2image']:
                st.success("‚úÖ Full PDF support (pdf2image + poppler)")
            elif deps['pymupdf_available']:
                st.info("‚ÑπÔ∏è Good PDF support (PyMuPDF)")
            else:
                st.warning("‚ö†Ô∏è Basic PDF support (PyPDF2 only)")
            
        # RAG Knowledge Base Status
        kb = RAGKnowledgeBase(st.session_state.rag_db_path)
        stats = kb.get_stats()
        
        st.header("üß† Knowledge Base")
        if not stats.get("error"):
            col1, col2 = st.columns(2)
            with col1:
                st.metric("üìö Documents", stats["total_documents"])
                st.metric("üìÑ Total Chunks", stats["total_chunks"])
            with col2:
                st.metric("üìù MD Chunks", stats.get("markdown_chunks", 0))
                st.metric("üìã Text Chunks", stats.get("text_chunks", 0))
            st.metric("üí¨ Chat Sessions", stats["total_chat_sessions"])
        else:
            st.error("‚ùå Knowledge base error")
        
        # Model selection
        st.subheader("ü§ñ Model Selection")
        selected_model = st.selectbox(
            "Choose AI Model:",
            options=list(AVAILABLE_MODELS.keys()),
            format_func=lambda x: f"{AVAILABLE_MODELS[x]['icon']} {AVAILABLE_MODELS[x]['name']}"
        )
        
        # Display model info
        model_info = AVAILABLE_MODELS[selected_model]
        st.info(f"**{model_info['name']}**\n\n{model_info['description']}\n\n**Best for:** {model_info['best_for']}")
        
        # Processing parameters
        st.subheader("üéØ Processing Parameters")
        temperature = st.slider("Temperature", 0.0, 1.0, 0.1, 0.1, help="Lower = more accurate, Higher = more creative")
        top_p = st.slider("Top P", 0.0, 1.0, 0.6, 0.1, help="Controls randomness of word selection")
        max_tokens = st.slider("Max Tokens", 1000, 16384, 12000, 500, help="Maximum length of generated text")
        
        # OCR specific settings
        st.subheader("üìÑ OCR Settings")
        prompt_type = st.selectbox(
            "Prompt Type:",
            ["default", "structure"],
            format_func=lambda x: "Default (Simple)" if x == "default" else "Structure (Complex Documents)",
            help="Structure mode better for tables, charts, and complex layouts"
        )
        
        output_format = st.selectbox(
            "Output Format:",
            ["markdown", "html", "json"],
            help="Choose output format - HTML best for tables, JSON for data processing"
        )
        
        # Advanced settings
        with st.expander("üîß Advanced Settings"):
            repetition_penalty = st.slider(
                "Repetition Penalty", 
                1.0, 2.0, 1.2, 0.1,
                help="Higher values reduce repetitive text"
            )
            image_quality = st.selectbox(
                "Image Quality", 
                ["high", "medium", "low"],
                index=1,
                help="Higher quality = better accuracy but slower processing"
            )
            batch_processing = st.checkbox(
                "Enable Batch Processing",
                help="Process multiple files at once"
            )
            
            # File size limit info
            st.info("üìè **File Limits:**\n- Max file size: 10MB\n- Supported: PDF, PNG, JPG, JPEG")

    # ===== Main content area with Tabs =====
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "üìÅ Upload & OCR", 
        "üß† Knowledge Base", 
        "üí¨ AI Chat", 
        "‚ú® Features", 
        "üìñ ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"
    ])

    # ---- Tab 1: Upload & Process (Original OCR) ----
    with tab1:
        st.header("üìÅ Upload Document for OCR")

        # File upload
        uploaded_files = st.file_uploader(
            "Choose files",
            type=['pdf', 'png', 'jpg', 'jpeg'],
            accept_multiple_files=batch_processing,
            help="Support: PDF, PNG, JPG, JPEG (Max: 10MB per file)"
        )
        
        # Preview uploaded files
        if uploaded_files:
            if isinstance(uploaded_files, list):
                total_size = sum(file.size for file in uploaded_files)
                st.success(f"‚úÖ {len(uploaded_files)} files uploaded ({total_size/1024/1024:.1f}MB total)")
                
                # Show file details
                for i, file in enumerate(uploaded_files[:5]):  # Show first 5
                    size_mb = file.size / 1024 / 1024
                    st.write(f"{i+1}. **{file.name}** ({size_mb:.1f}MB, {file.type})")
                    
                if len(uploaded_files) > 5:
                    st.write(f"... and {len(uploaded_files) - 5} more files")
                    
            else:
                size_mb = uploaded_files.size / 1024 / 1024
                st.success(f"‚úÖ File uploaded: **{uploaded_files.name}** ({size_mb:.1f}MB)")
                
                # Show preview for single file
                if uploaded_files.type.startswith('image'):
                    try:
                        image = Image.open(uploaded_files)
                        st.image(image, caption="Uploaded Image", use_column_width=True)
                    except Exception as e:
                        st.error(f"‚ùå Cannot preview image: {str(e)}")
                elif uploaded_files.type == 'application/pdf':
                    st.info("üìÑ PDF file uploaded - preview will be shown during processing")
        
        # Process button with validation
        col1, col2 = st.columns([3, 1])
        
        with col1:
            if st.button("üöÄ Process Document(s)", type="primary", disabled=not uploaded_files):
                if uploaded_files:
                    # Validate total processing load
                    files_to_check = uploaded_files if isinstance(uploaded_files, list) else [uploaded_files]
                    total_size = sum(file.size for file in files_to_check)
                    
                    # Warn for large processing jobs
                    if total_size > 50 * 1024 * 1024:  # 50MB
                        st.warning("‚ö†Ô∏è Large file size detected. Processing may take several minutes.")
                    
                    if len(files_to_check) > 5:
                        st.warning("‚ö†Ô∏è Processing multiple files. This may take some time.")
                    
                    # Start processing
                    try:
                        process_documents(uploaded_files, selected_model, {
                            'temperature': temperature,
                            'top_p': top_p,
                            'max_tokens': max_tokens,
                            'repetition_penalty': repetition_penalty,
                            'prompt_type': prompt_type,
                            'output_format': output_format,
                            'image_quality': image_quality
                        })
                    except Exception as e:
                        st.error(f"‚ùå Processing failed: {str(e)}")
                        st.error("Please check the error logs above and try again.")
                else:
                    st.error("‚ùå Please upload at least one file!")
        
        with col2:
            if st.button("üîÑ Clear Files"):
                st.rerun()

    # ---- Tab 2: Enhanced Knowledge Base Management ----
    with tab2:
        st.header("üß† Enhanced Knowledge Base Management")
        
        kb = RAGKnowledgeBase(st.session_state.rag_db_path)
        stats = kb.get_stats()
        
        # Display enhanced stats
        if not stats.get("error"):
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("üìö Total Documents", stats["total_documents"])
            with col2:
                st.metric("üìÑ Total Chunks", stats["total_chunks"])
            with col3:
                st.metric("üìù Markdown Chunks", stats.get("markdown_chunks", 0))
            with col4:
                st.metric("üí¨ Chat Sessions", stats["total_chat_sessions"])
        
        # Knowledge Base Actions
        st.markdown("---")
        st.subheader("üîß Knowledge Base Actions")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("üîÑ Re-index All Documents", help="Recreate embeddings for all documents"):
                if stats.get("total_documents", 0) > 0:
                    with st.spinner("Re-indexing documents..."):
                        if kb.reindex_documents():
                            st.success("‚úÖ Successfully re-indexed all documents!")
                            st.rerun()
                else:
                    st.info("‚ÑπÔ∏è No documents to re-index")
        
        with col2:
            if st.button("üóëÔ∏è Reset Knowledge Base", help="Delete ALL documents and chat history"):
                # Double confirmation system
                if 'reset_confirm_step' not in st.session_state:
                    st.session_state.reset_confirm_step = 0
                
                if st.session_state.reset_confirm_step == 0:
                    st.session_state.reset_confirm_step = 1
                    st.warning("‚ö†Ô∏è First confirmation: Click again to proceed to final confirmation.")
                elif st.session_state.reset_confirm_step == 1:
                    st.session_state.reset_confirm_step = 2
                    st.error("üö® FINAL WARNING: This will DELETE ALL data! Click once more to confirm.")
                else:
                    # Final confirmation - actually reset
                    with st.spinner("Resetting knowledge base..."):
                        if kb.reset_knowledge_base():
                            st.success("‚úÖ Knowledge base has been completely reset!")
                            st.session_state.chat_history = []  # Clear chat history
                            st.session_state.reset_confirm_step = 0  # Reset confirmation
                            st.rerun()
                        else:
                            st.error("‚ùå Failed to reset knowledge base")
                            st.session_state.reset_confirm_step = 0
        
        st.markdown("---")
        
        # Add documents manually
        st.subheader("üì§ Add Documents to Knowledge Base")
        
        # Method 1: Upload Markdown files
        st.write("**Method 1: Upload Markdown Files (Smart Chunking)**")
        st.info("üìù Markdown files will be processed with header-aware chunking for better context preservation.")
        
        markdown_files = st.file_uploader(
            "Upload Markdown files",
            type=['md', 'txt'],
            accept_multiple_files=True,
            help="Upload .md or .txt files - .md files get smart header-based chunking"
        )
        
        if markdown_files and st.button("‚ûï Add Markdown Files"):
            added_count = 0
            progress_bar = st.progress(0)
            
            for i, file in enumerate(markdown_files):
                progress_bar.progress((i + 1) / len(markdown_files))
                try:
                    content = file.read().decode('utf-8')
                    file_type = "markdown" if file.name.endswith('.md') else "text"
                    
                    if kb.add_document(
                        filename=file.name,
                        content=content,
                        metadata={
                            "source": "manual_upload", 
                            "file_type": file_type,
                            "upload_method": "file_upload"
                        }
                    ):
                        added_count += 1
                except Exception as e:
                    st.error(f"‚ùå Error adding {file.name}: {str(e)}")
            
            if added_count > 0:
                st.success(f"‚úÖ Added {added_count} files to knowledge base with smart chunking!")
                st.rerun()
        
        # Method 2: Direct text input
        st.write("**Method 2: Direct Text Input**")
        with st.form("add_text_form"):
            doc_title = st.text_input("Document Title", placeholder="Enter document title")
            doc_content = st.text_area(
                "Document Content",
                height=200,
                placeholder="Paste or type your document content here... (Use Markdown format for better structure)"
            )
            
            # Choose file format
            content_format = st.selectbox(
                "Content Format:",
                ["markdown", "text"],
                help="Markdown format enables smart header-based chunking"
            )
            
            submitted = st.form_submit_button("‚ûï Add to Knowledge Base")
            if submitted and doc_title and doc_content:
                filename = f"{doc_title}.{content_format}"
                if kb.add_document(
                    filename=filename,
                    content=doc_content,
                    metadata={
                        "source": "manual_input",
                        "file_type": content_format,
                        "upload_method": "direct_input"
                    }
                ):
                    st.success(f"‚úÖ Added '{doc_title}' to knowledge base!")
                    st.rerun()
        
        st.markdown("---")
        
        # Enhanced knowledge base browser
        st.subheader("üìñ Browse Knowledge Base")
        
        if stats.get("total_documents", 0) > 0:
            # Get document list with enhanced info
            conn = sqlite3.connect(st.session_state.rag_db_path)
            cursor = conn.cursor()
            cursor.execute("""
                SELECT 
                    filename, 
                    COUNT(*) as chunk_count, 
                    MAX(created_at) as latest,
                    AVG(header_level) as avg_header_level,
                    COUNT(DISTINCT section_title) as sections
                FROM documents 
                GROUP BY filename 
                ORDER BY latest DESC
            """)
            docs = cursor.fetchall()
            conn.close()
            
            # Display documents with enhanced info
            for filename, chunk_count, created_at, avg_header_level, sections in docs:
                chunk_type = "üìù MD" if filename.endswith('.md') else "üìã TXT"
                with st.expander(f"{chunk_type} {filename} ({chunk_count} chunks, {sections} sections)"):
                    col1, col2 = st.columns(2)
                    with col1:
                        st.write(f"**Created:** {created_at}")
                        st.write(f"**Chunks:** {chunk_count}")
                        st.write(f"**Sections:** {sections}")
                    with col2:
                        if avg_header_level and avg_header_level > 0:
                            st.write(f"**Avg Header Level:** {avg_header_level:.1f}")
                        st.write(f"**Type:** {'Markdown' if filename.endswith('.md') else 'Plain Text'}")
                    
                    # Show first chunk as preview
                    conn = sqlite3.connect(st.session_state.rag_db_path)
                    cursor = conn.cursor()
                    cursor.execute("""
                        SELECT content, section_title 
                        FROM documents 
                        WHERE filename = ? 
                        ORDER BY chunk_id 
                        LIMIT 1
                    """, (filename,))
                    preview = cursor.fetchone()
                    if preview:
                        content, section_title = preview
                        st.write("**Preview:**")
                        if section_title:
                            st.write(f"*Section: {section_title}*")
                        st.text(content[:300] + "..." if len(content) > 300 else content)
                    conn.close()
                    
                    # Delete button
                    if st.button(f"üóëÔ∏è Delete {filename}", key=f"del_{filename}"):
                        conn = sqlite3.connect(st.session_state.rag_db_path)
                        cursor = conn.cursor()
                        cursor.execute("DELETE FROM documents WHERE filename = ?", (filename,))
                        conn.commit()
                        conn.close()
                        st.success(f"‚úÖ Deleted {filename}")
                        st.rerun()
        else:
            st.info("üìù No documents in knowledge base yet. Add some documents to get started!")

    # ---- Tab 3: Enhanced AI Chat with RAG ----
    with tab3:
        st.header("üí¨ Enhanced AI Chat with Knowledge Base")
        
        # Check if knowledge base has content
        if stats.get("total_documents", 0) == 0:
            st.warning("‚ö†Ô∏è No documents in knowledge base. Please add documents in the Knowledge Base tab first.")
            return
        
        # Enhanced chat settings
        st.subheader("‚öôÔ∏è Chat Settings")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            chat_model = st.selectbox(
                "Chat Model:",
                options=[k for k in AVAILABLE_MODELS.keys() if k != "scb10x/typhoon-ocr-7b:latest"],
                index=0,
                format_func=lambda x: f"{AVAILABLE_MODELS[x]['icon']} {AVAILABLE_MODELS[x]['name']}",
                help="Choose model for answering questions"
            )
        
        with col2:
            use_mmr = st.checkbox(
                "Use MMR Search", 
                value=True,
                help="Maximal Marginal Relevance for diverse, relevant results"
            )
        
        with col3:
            strict_mode = st.checkbox(
                "Strict Mode", 
                value=True,
                help="Refuse to answer if no relevant information found"
            )
        
        # Advanced search settings
        with st.expander("üîß Advanced Search Settings"):
            col1, col2 = st.columns(2)
            with col1:
                top_k = st.slider("Results to retrieve", 3, 10, 5)
                min_similarity = st.slider("Min similarity threshold", 0.1, 0.8, 0.3, 0.1)
            with col2:
                diversity_weight = st.slider("MMR diversity weight", 0.0, 1.0, 0.3, 0.1)
                st.write(f"**Current Settings:**")
                st.write(f"- Model: {AVAILABLE_MODELS[chat_model]['name']}")
                st.write(f"- Search: {'MMR' if use_mmr else 'Similarity'}")
                st.write(f"- Mode: {'Strict' if strict_mode else 'Flexible'}")
        
        st.markdown("---")
        
        # Chat interface
        st.subheader("üó£Ô∏è Ask Questions About Your Documents")
        
        # Display chat history with enhanced formatting
        for i, (question, answer, context) in enumerate(st.session_state.chat_history):
            # User message
            st.markdown(f"""
            <div class="chat-message user-message">
                <strong>ü§î You:</strong><br>
                {question}
            </div>
            """, unsafe_allow_html=True)
            
            # Assistant message
            st.markdown(f"""
            <div class="chat-message assistant-message">
                <strong>ü§ñ Assistant ({AVAILABLE_MODELS.get(chat_model, {}).get('name', 'Unknown')}):</strong><br>
                {answer.replace('üìö **‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:**', '<br><strong>üìö ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:</strong>')}
            </div>
            """, unsafe_allow_html=True)
            
            # Show context sources with enhanced display
            if context:
                with st.expander(f"üìö Source Context (Chat {i+1})", expanded=False):
                    for j, (filename, content, similarity, section_title) in enumerate(context):
                        st.markdown(f"""
                        <div class="source-reference">
                            <strong>üìÑ Source {j+1}: {filename}</strong><br>
                            <em>Section: {section_title if section_title else 'N/A'} | Relevance: {similarity:.1%}</em><br>
                            <small>{content[:200]}...</small>
                        </div>
                        """, unsafe_allow_html=True)
        
        # Enhanced chat input
        with st.form("enhanced_chat_form", clear_on_submit=True):
            user_question = st.text_area(
                "Ask a question:",
                placeholder="What would you like to know about your documents? \n\nTips:\n- Be specific about what you're looking for\n- Mention document names if you want to focus on specific files\n- Ask for comparisons or summaries across documents",
                height=100,
                key="chat_input"
            )
            
            col1, col2, col3, col4 = st.columns([2, 2, 2, 2])
            with col1:
                ask_button = st.form_submit_button("üöÄ Ask Question", type="primary")
            with col2:
                clear_button = st.form_submit_button("üóëÔ∏è Clear Chat")
            with col3:
                export_button = st.form_submit_button("üíæ Export Chat")
            with col4:
                search_only = st.form_submit_button("üîç Search Only")
            
            if clear_button:
                st.session_state.chat_history = []
                st.rerun()
            
            if export_button and st.session_state.chat_history:
                # Export chat history
                chat_export = "# Chat History Export\n\n"
                for i, (q, a, ctx) in enumerate(st.session_state.chat_history):
                    chat_export += f"## Question {i+1}\n**Q:** {q}\n\n**A:** {a}\n\n"
                    if ctx:
                        chat_export += "**Sources:**\n"
                        for j, (fname, content, sim, section) in enumerate(ctx):
                            chat_export += f"- {fname} ({section or 'N/A'}) - {sim:.1%}\n"
                        chat_export += "\n"
                
                st.download_button(
                    "üì• Download Chat History",
                    chat_export,
                    "chat_history.md",
                    "text/markdown"
                )
        
        if search_only and user_question:
            # Search only mode - show results without generating answer
            with st.spinner("üîç Searching knowledge base..."):
                if use_mmr:
                    # Use enhanced search method directly
                    context_docs = kb.search_similar(
                        user_question, 
                        top_k=top_k, 
                        use_mmr=True, 
                        min_similarity=min_similarity
                    )
                else:
                    context_docs = kb.search_similar(
                        user_question, 
                        top_k=top_k, 
                        use_mmr=False, 
                        min_similarity=min_similarity
                    )
                
                if context_docs:
                    st.success(f"‚úÖ Found {len(context_docs)} relevant results:")
                    for i, (filename, content, similarity, section_title) in enumerate(context_docs):
                        st.markdown(f"""
                        <div class="knowledge-card">
                            <strong>üìÑ Result {i+1}: {filename}</strong><br>
                            <em>Section: {section_title or 'N/A'} | Relevance: {similarity:.1%}</em><br>
                            <small>{content[:300]}...</small>
                        </div>
                        """, unsafe_allow_html=True)
                else:
                    st.warning("‚ö†Ô∏è No relevant information found in knowledge base")
        
        elif ask_button and user_question:
            with st.spinner("üîç Searching knowledge base and generating response..."):
                # Enhanced search with user settings
                context_docs = kb.search_similar(
                    user_question, 
                    top_k=top_k, 
                    use_mmr=use_mmr, 
                    min_similarity=min_similarity
                )
                
                if context_docs or not strict_mode:
                    # Generate RAG response with selected model
                    response = generate_rag_response(
                        user_question, 
                        context_docs, 
                        chat_model,
                        strict_mode=strict_mode
                    )
                    
                    if response:
                        # Add to chat history with enhanced context info
                        enhanced_context = [
                            (filename, content, similarity, section_title)
                            for filename, content, similarity, section_title in context_docs[:3]
                        ]
                        
                        st.session_state.chat_history.append((
                            user_question,
                            response,
                            enhanced_context
                        ))
                        
                        # Save to database with enhanced metadata
                        conn = sqlite3.connect(st.session_state.rag_db_path)
                        cursor = conn.cursor()
                        cursor.execute('''
                            INSERT INTO chat_sessions (session_id, question, answer, context)
                            VALUES (?, ?, ?, ?)
                        ''', (
                            "enhanced_session",
                            user_question,
                            response,
                            json.dumps({
                                "context": [(doc[0], doc[1][:200], doc[2], doc[3]) for doc in enhanced_context],
                                "model": chat_model,
                                "settings": {
                                    "use_mmr": use_mmr,
                                    "strict_mode": strict_mode,
                                    "top_k": top_k,
                                    "min_similarity": min_similarity
                                }
                            })
                        ))
                        conn.commit()
                        conn.close()
                        
                        st.rerun()
                    else:
                        st.error("‚ùå Failed to generate response")
                else:
                    st.warning("‚ö†Ô∏è No relevant information found in knowledge base (try lowering similarity threshold or disabling strict mode)")

    # ---- Tab 4: Features ----
    with tab4:
        st.header("‚ú® Enhanced Features with Smart RAG")
        
        # Enhanced feature cards
        features = [
            {
                "icon": "üå™Ô∏è",
                "title": "Advanced OCR Processing",
                "description": "‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢ AI ‡∏ó‡∏µ‡πà‡∏ó‡∏±‡∏ô‡∏™‡∏°‡∏±‡∏¢",
                "items": [
                    "Thai-English OCR with high accuracy",
                    "Complex document structure recognition", 
                    "Batch processing for multiple files",
                    "Multiple output formats (MD, HTML, JSON)"
                ]
            },
            {
                "icon": "üß†", 
                "title": "Smart Knowledge Base with MMR",
                "description": "‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö MMR",
                "items": [
                    "Auto-import OCR results to knowledge base",
                    "Smart markdown chunking with header awareness",
                    "MMR search for balanced relevance and diversity",
                    "Configurable similarity thresholds and strict mode"
                ]
            },
            {
                "icon": "üí¨",
                "title": "Enhanced AI Q&A System",
                "description": "‡∏ñ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏Å‡∏±‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢ AI ‡∏ó‡∏µ‡πà‡∏â‡∏•‡∏≤‡∏î‡∏Ç‡∏∂‡πâ‡∏ô",
                "items": [
                    "Context-aware responses with source attribution",
                    "Multiple model support (Typhoon2, Qwen2.5, Nomic)",
                    "Strict mode prevents hallucination",
                    "Enhanced chat history with export functionality"
                ]
            },
            {
                "icon": "üîß",
                "title": "Advanced Management Tools", 
                "description": "‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ",
                "items": [
                    "Re-indexing with updated embeddings",
                    "Safe knowledge base reset with double confirmation",
                    "Document browser with section tracking",
                    "Performance metrics and chunk type analysis"
                ]
            }
        ]
        
        for feature in features:
            st.markdown(f"""
            <div class="feature-card">
                <h3>{feature['icon']} {feature['title']}</h3>
                <p>{feature['description']}</p>
                <ul>
                    {' '.join([f'<li>‚úì {item}</li>' for item in feature['items']])}
                </ul>
            </div>
            """, unsafe_allow_html=True)
        
        # Enhanced workflow diagram
        st.subheader("üîÑ Smart RAG Workflow")
        st.markdown("""
        ```mermaid
        graph TD
            A[üìÑ Upload Document] --> B[üå™Ô∏è OCR Processing]
            B --> C[üìù Extract Text/Markdown]
            C --> D{Add to Knowledge Base?}
            D -->|Yes| E[üß† Smart Chunking]
            E --> F[Header-Aware MD] --> G[üîó Generate Embeddings]
            E --> H[Regular Text] --> G
            G --> I[üíæ Store in RAG DB]
            D -->|No| J[üíæ Download Results]
            
            K[üì§ Manual MD Upload] --> E
            L[‚úçÔ∏è Direct Input] --> E
            
            I --> M[üí¨ Enhanced Chat]
            M --> N{Search Method}
            N -->|MMR| O[üéØ MMR Search]
            N -->|Similarity| P[üìä Cosine Search]
            O --> Q[ü§ñ Generate Answer]
            P --> Q
            Q --> R{Strict Mode?}
            R -->|Yes| S[‚úÖ Answer with Sources]
            R -->|No| T[üîÑ Flexible Response]
        ```
        """)
        
        # Enhanced model comparison
        st.subheader("‚ö° Enhanced Model Performance")
        
        performance_data = {
            "Model": ["Typhoon OCR 7B", "Qwen2.5 14B", "Typhoon2 8B", "Nomic Embed"],
            "Thai OCR": ["‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê", "‚≠ê‚≠ê‚≠ê", "‚≠ê‚≠ê‚≠ê‚≠ê", "N/A"],
            "English OCR": ["‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê", "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê", "‚≠ê‚≠ê‚≠ê‚≠ê", "N/A"],
            "Complex Tables": ["‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê", "‚≠ê‚≠ê‚≠ê‚≠ê", "‚≠ê‚≠ê‚≠ê", "N/A"],
            "Thai Q&A": ["‚≠ê‚≠ê‚≠ê‚≠ê", "‚≠ê‚≠ê‚≠ê‚≠ê", "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê", "N/A"],
            "Processing Speed": ["‚≠ê‚≠ê‚≠ê‚≠ê", "‚≠ê‚≠ê", "‚≠ê‚≠ê‚≠ê‚≠ê", "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê"],
            "Best For": ["OCR Tasks", "Complex Reasoning", "Thai Content", "Embeddings"]
        }
        
        st.table(performance_data)
        
        # Smart RAG benefits
        st.subheader("üéØ Smart RAG System Benefits")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            **üß† Intelligent Processing:**
            - Header-aware markdown chunking
            - Context preservation across chunks
            - Smart overlap for continuity
            - Section-based organization
            
            **üîç Advanced Search:**
            - MMR for relevance + diversity
            - Configurable similarity thresholds
            - Multi-model support
            - Source attribution with sections
            """)
        
        with col2:
            st.markdown("""
            **‚öôÔ∏è Enhanced Management:**
            - Re-indexing capabilities
            - Safe reset with confirmations
            - Document type tracking
            - Performance monitoring
            
            **üéØ Quality Assurance:**
            - Strict mode prevents hallucination
            - Source transparency
            - Chat history export
            - Embedding version control
            """)
        
        # Technical specifications
        st.subheader("üî¨ Technical Specifications")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            **Chunking Strategy:**
            - Markdown: Header-aware, 1500 char max
            - Text: Sentence-boundary, 1000 char + 200 overlap
            - Context preservation with header hierarchy
            - Smart overlap for continuity
            """)
        
        with col2:
            st.markdown("""
            **Search Algorithm:**
            - MMR with configurable diversity (default: 0.3)
            - Cosine similarity baseline
            - Minimum similarity filtering (default: 0.3)
            - Top-K retrieval (configurable: 3-10)
            """)

    # ---- Tab 5: Comprehensive User Guide ----
    with tab5:
        st.header("üìñ ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Typhoon OCR with Enhanced RAG")
        
        st.markdown("""
## üåü ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡∏°‡πà

Typhoon OCR with Enhanced RAG ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏° **OCR ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á** ‡∏Å‡∏±‡∏ö **Smart RAG System** ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ:

1. **üìÑ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£** - ‡πÅ‡∏õ‡∏•‡∏á PDF/‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ AI
2. **üß† ‡∏ê‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞** - ‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö semantic
3. **üí¨ ‡∏ñ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á** - ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ MMR algorithm
4. **üîß ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á** - ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏î‡∏π‡πÅ‡∏•‡∏£‡∏∞‡∏ö‡∏ö

---

## 
                st.warning(f"‚ö†Ô∏è Error processing page {page_num + 1}: {str(e)}")
                # Create placeholder image
                img = Image.new('RGB', (800, 1000), color='white')
                images.append(img)
        
        return images
    except Exception as e:
        st.error(f"‚ùå Error using PyPDF2 fallback: {str(e)}")
        return []

def convert_pdf_with_pymupdf(pdf_file, quality="medium") -> List[Image.Image]:
    """Convert PDF using PyMuPDF (fitz)"""
    try:
        import fitz
        
        # Set DPI based on quality
        dpi = {"high": 300, "medium": 200, "low": 150}.get(quality, 200)
        
        pdf_file.seek(0)
        pdf_bytes = pdf_file.read()
        
        # Open PDF from bytes
        pdf_doc = fitz.open(stream=pdf_bytes, filetype="pdf")
        images = []
        
        for page_num in range(pdf_doc.page_count):
            try:
                page = pdf_doc[page_num]
                
                # Create matrix for scaling
                zoom = dpi / 72.0  # 72 DPI is default
                matrix = fitz.Matrix(zoom, zoom)
                
                # Render page to image
                pix = page.get_pixmap(matrix=matrix)
                img_data = pix.tobytes("png")
                
                # Convert to PIL Image
                img = Image.open(BytesIO(img_data))
                images.append(img)
                
            except Exception as e:
