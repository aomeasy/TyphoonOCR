# -*- coding: utf-8 -*-
"""
rag_addons.py
‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏¢‡∏≤‡∏¢ RAG ‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°:
- RAGKnowledgeBaseX: chunk Markdown ‡∏£‡∏π‡πâ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ + MMR retrieval + re-index/reset
- generate_rag_response_strict: ‡∏ï‡∏≠‡∏ö‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î ‡∏°‡∏µ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á [1],[2] ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏î‡∏≤
- extractive_answer: ‡∏î‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ï‡∏£‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ LLM ‡∏™‡∏£‡∏∏‡∏õ)
- UI helper ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤ Chat/Maintenance ‡πÉ‡∏´‡∏°‡πà
"""
from __future__ import annotations
import streamlit as st
import requests
import json
import sqlite3
import hashlib
import pickle
import re
import math
from typing import Optional, Dict, Any, List, Tuple
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# ===== Default endpoints (‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ UI) =====
EMBEDDING_API_URL_DEFAULT = "http://209.15.123.47:11434/api/embeddings"
OLLAMA_API_URL_DEFAULT    = "http://209.15.123.47:11434/api/generate"
EMBEDDING_MODEL_DEFAULT   = "nomic-embed-text:latest"

# ==================== KB Class (Enhanced) ====================

class RAGKnowledgeBaseX:
    """
    ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏™‡∏£‡∏¥‡∏°: ‡πÉ‡∏ä‡πâ‡∏ï‡∏≤‡∏£‡∏≤‡∏á documents/chat_sessions ‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏î‡πâ, chunk Markdown, MMR, reindex/reset
    ‡πÑ‡∏°‡πà‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Ñ‡∏•‡∏≤‡∏™‡πÄ‡∏î‡∏¥‡∏°‡πÉ‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå
    """
    def __init__(
        self,
        db_path: str = "typhoon_rag_knowledge.db",
        embedding_api_url: str = EMBEDDING_API_URL_DEFAULT,
        embedding_model: str = EMBEDDING_MODEL_DEFAULT
    ):
        self.db_path = db_path
        self.embedding_api_url = embedding_api_url
        self.embedding_model = embedding_model
        self._init_db()

    def set_embedding_model(self, model: str):
        if model:
            self.embedding_model = model

    def set_embedding_api(self, url: str):
        if url:
            self.embedding_api_url = url

    def _init_db(self):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS documents (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                filename TEXT NOT NULL,
                content TEXT NOT NULL,
                embedding BLOB,
                chunk_id INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                file_hash TEXT,
                metadata TEXT
            )
        ''')
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS chat_sessions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_id TEXT,
                question TEXT,
                answer TEXT,
                context TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        conn.commit(); conn.close()

    # ---------- Embedding ----------
    def get_embedding(self, text: str) -> Optional[List[float]]:
        try:
            resp = requests.post(
                self.embedding_api_url,
                json={"model": self.embedding_model, "prompt": text},
                timeout=60
            )
            resp.raise_for_status()
            return resp.json().get("embedding", [])
        except Exception as e:
            st.error(f"‚ùå Embedding error: {e}")
            return None

    # ---------- Chunkers ----------
    def chunk_markdown(self, md_text: str, max_chars: int = 1200, overlap: int = 120) -> List[Dict]:
        """
        ‡πÅ‡∏ö‡πà‡∏á Markdown ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ (#..######) ‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡∏ï‡∏±‡∏î‡πÅ‡∏ö‡∏ö‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á (‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠)
        return: [{"text": "...", "section": "‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠"}]
        """
        if not md_text:
            return []
        # ‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏ï‡∏≤‡∏° header
        blocks = re.split(r'(?m)^(?=\s{0,3}#{1,6}\s)', md_text)
        chunks: List[Dict] = []

        def sliding(text: str, section_path: str):
            text = text.strip()
            if not text:
                return
            if len(text) <= max_chars:
                chunks.append({"text": text, "section": section_path})
                return
            start = 0
            while start < len(text):
                end = min(len(text), start + max_chars)
                piece = text[start:end]
                chunks.append({"text": piece, "section": section_path})
                if end >= len(text):
                    break
                start = max(0, end - overlap)

        for b in blocks:
            lines = b.strip().splitlines()
            if not lines:
                continue
            if re.match(r'^\s{0,3}#{1,6}\s', lines[0]):  # header
                header_line = lines[0].strip()
                header_text = re.sub(r'^\s{0,3}#{1,6}\s', '', header_line).strip()
                body = "\n".join(lines[1:]).strip()
                section_path = header_text
                sliding(body if body else header_text, section_path)
            else:
                sliding(b, "root")

        return chunks

    def chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
        if len(text) <= chunk_size:
            return [text]
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            if end < len(text):
                sentence_ends = ['. ', '! ', '? ', '\n\n', '„ÄÇ', 'ÔºÅ', 'Ôºü']
                best_break = end
                for i in range(min(100, len(text) - end)):
                    for ending in sentence_ends:
                        if text[end + i:end + i + len(ending)] == ending:
                            best_break = end + i + len(ending)
                            break
                    if best_break != end:
                        break
                end = best_break
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            start = end - overlap
            if start >= len(text):
                break
        return chunks

    # ---------- Add Document ----------
    def add_document(self, filename: str, content: str, metadata: Dict = None,
                     chunk_size: int = 1200, overlap: int = 120) -> bool:
        """
        ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏ö‡∏ö‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô .md (‡πÉ‡∏ä‡πâ chunk_markdown)
        """
        try:
            file_hash = hashlib.md5(content.encode('utf-8')).hexdigest()
            conn = sqlite3.connect(self.db_path); cursor = conn.cursor()
            cursor.execute("SELECT id FROM documents WHERE file_hash=?", (file_hash,))
            if cursor.fetchone():
                conn.close()
                st.warning(f"‚ö†Ô∏è Document {filename} already exists")
                return False

            if filename.lower().endswith(".md"):
                md_chunks = self.chunk_markdown(content, max_chars=chunk_size, overlap=overlap)
                pairs = [(c["text"], c["section"]) for c in md_chunks]
            else:
                pairs = [(x, "") for x in self.chunk_text(content, chunk_size, overlap)]

            progress = st.progress(0.0); msg = st.empty()
            for i, (txt, section) in enumerate(pairs):
                msg.text(f"Embedding chunk {i+1}/{len(pairs)}‚Ä¶")
                emb = self.get_embedding(txt)
                if emb:
                    cursor.execute("""
                        INSERT INTO documents (filename, content, embedding, chunk_id, file_hash, metadata)
                        VALUES (?, ?, ?, ?, ?, ?)
                    """, (
                        filename,
                        txt,
                        pickle.dumps(emb),
                        i,
                        file_hash,
                        json.dumps({**(metadata or {}), "section": section, "embedding_model": self.embedding_model}, ensure_ascii=False)
                    ))
                progress.progress((i+1)/max(1, len(pairs)))
            conn.commit(); conn.close()
            msg.text(f"‚úÖ Added {len(pairs)} chunks from {filename}")
            return True
        except Exception as e:
            st.error(f"‚ùå Add document error: {e}")
            return False

    # ---------- Search (MMR) ----------
    def _cosine(self, a, b) -> float:
        dot = sum(x*y for x, y in zip(a, b))
        na = math.sqrt(sum(x*x for x in a)); nb = math.sqrt(sum(y*y for y in b))
        return 0.0 if na == 0 or nb == 0 else dot/(na*nb)

    def _mmr_rank(self, query_vec, doc_vecs, lambda_mult=0.7, top_k=5):
        selected = []
        candidates = list(range(len(doc_vecs)))
        while candidates and len(selected) < top_k:
            best_idx, best_score = None, -1e9
            for i in candidates:
                rel = self._cosine(query_vec, doc_vecs[i])
                div = max((self._cosine(doc_vecs[i], doc_vecs[j]) for j in selected), default=0.0)
                score = lambda_mult*rel - (1 - lambda_mult)*div
                if score > best_score:
                    best_idx, best_score = i, score
            selected.append(best_idx); candidates.remove(best_idx)
        return selected

    def search_similar(self, query: str, top_k: int = 5, use_mmr: bool = True, lambda_mult: float = 0.7) -> List[Tuple[str, str, float]]:
        try:
            q = self.get_embedding(query)
            if not q:
                return []
            conn = sqlite3.connect(self.db_path); cursor = conn.cursor()
            cursor.execute("SELECT filename, content, embedding FROM documents")
            rows = cursor.fetchall(); conn.close()

            if not rows:
                return []

            vecs = [pickle.loads(r[2]) for r in rows]
            items = [(r[0], r[1]) for r in rows]

            if use_mmr:
                idxs = self._mmr_rank(q, vecs, lambda_mult=lambda_mult, top_k=top_k)
                scored = []
                for i in idxs:
                    sim = float(cosine_similarity([q], [vecs[i]])[0][0])
                    fn, txt = items[i]
                    scored.append((fn, txt, sim))
                scored.sort(key=lambda x: x[2], reverse=True)
                return scored
            else:
                sims = cosine_similarity([q], vecs)[0]
                order = np.argsort(-sims)[:top_k]
                return [(items[i][0], items[i][1], float(sims[i])) for i in order]
        except Exception as e:
            st.error(f"‚ùå Search error: {e}")
            return []

    # ---------- Stats / Maintenance ----------
    def get_stats(self) -> Dict[str, Any]:
        try:
            conn = sqlite3.connect(self.db_path); cursor = conn.cursor()
            cursor.execute("SELECT COUNT(DISTINCT filename) FROM documents")
            total_docs = cursor.fetchone()[0]
            cursor.execute("SELECT COUNT(*) FROM documents")
            total_chunks = cursor.fetchone()[0]
            cursor.execute("SELECT COUNT(*) FROM chat_sessions")
            total_chats = cursor.fetchone()[0]
            conn.close()
            return {"total_documents": total_docs, "total_chunks": total_chunks, "total_chat_sessions": total_chats}
        except Exception as e:
            return {"error": str(e)}

    def reset_kb(self):
        conn = sqlite3.connect(self.db_path); cursor = conn.cursor()
        cursor.execute("DELETE FROM documents")
        cursor.execute("DELETE FROM chat_sessions")
        conn.commit(); conn.close()

    def reindex_documents(self, filenames: Optional[List[str]] = None, chunk_size: int = 1200, overlap: int = 120):
        """
        ‡∏£‡∏ß‡∏°‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏î‡∏¥‡∏°‡∏ï‡∏≤‡∏° chunk_id ‡πÅ‡∏•‡πâ‡∏ß‡∏ù‡∏±‡∏á‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÉ‡∏´‡∏°‡πà‡∏î‡πâ‡∏ß‡∏¢ chunker ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
        """
        conn = sqlite3.connect(self.db_path); cursor = conn.cursor()
        if not filenames:
            cursor.execute("SELECT DISTINCT filename FROM documents")
            filenames = [r[0] for r in cursor.fetchall()]

        for fn in filenames:
            cursor.execute("SELECT content FROM documents WHERE filename=? ORDER BY chunk_id ASC", (fn,))
            rows = cursor.fetchall()
            if not rows:
                continue
            full_text = "\n\n".join(r[0] for r in rows)
            cursor.execute("DELETE FROM documents WHERE filename=?", (fn,))
            conn.commit()
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà
            self.add_document(fn, full_text, metadata={"reindexed": True}, chunk_size=chunk_size, overlap=overlap)
        conn.close()

# ==================== Answering Functions ====================

def generate_rag_response_strict(
    query: str,
    context_docs: List[Tuple[str, str, float]],
    llm_model: str = "qwen2.5:14b",
    ollama_api_url: str = OLLAMA_API_URL_DEFAULT
) -> Optional[str]:
    """
    ‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î + ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á [1],[2] / ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò
    """
    try:
        cites = []
        for i, d in enumerate(context_docs[:5], 1):
            fn, txt, sim = d
            cites.append(f"[{i}] {fn} (sim={sim:.3f})\n{txt}")
        context_text = "\n\n---\n\n".join(cites) if cites else "(no context)"

        prompt = f"""[SYSTEM]
‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏Ñ‡∏•‡∏±‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£ ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô "‡∏ö‡∏£‡∏¥‡∏ö‡∏ó" ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
‡∏´‡πâ‡∏≤‡∏°‡∏Ñ‡∏≤‡∏î‡πÄ‡∏î‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏ï‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏û‡∏ö ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤"
‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏£‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÅ‡∏•‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏≠‡πâ‡∏≤‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡∏ö [1],[2] ‡∏ï‡∏£‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏≠‡πâ‡∏≤‡∏á‡∏ñ‡∏∂‡∏á

[USER]
‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {query}

‡∏ö‡∏£‡∏¥‡∏ö‡∏ó:
{context_text}

‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:
- ‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
- ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏≠‡πâ‡∏≤‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà [index] ‡πÄ‡∏ä‡πà‡∏ô [1]
- ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡∏û‡∏¥‡∏°‡∏û‡πå: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤

‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:
"""
        resp = requests.post(
            ollama_api_url,
            json={"model": llm_model, "prompt": prompt, "temperature": 0.1, "top_p": 0.6, "num_predict": 800, "stream": False},
            timeout=120
        )
        resp.raise_for_status()
        return resp.json().get("response", "")
    except Exception as e:
        st.error(f"‚ùå Strict answer error: {e}")
        return None


def extractive_answer(
    query: str,
    context_docs: List[Tuple[str, str, float]],
    kb: RAGKnowledgeBaseX,
    top_n: int = 3
) -> Optional[str]:
    """
    Extractive QA: ‡∏î‡∏∂‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ LLM ‡πÅ‡∏ï‡πà‡∏á/‡∏™‡∏£‡∏∏‡∏õ
    """
    try:
        qvec = kb.get_embedding(query)
        if not qvec:
            return None
        sent_re = re.compile(r'(?<=[\.\?!„ÄÇÔºÅÔºü])\s+|\n+')
        cands = []
        for fn, text, sim in context_docs[:5]:
            sentences = [s.strip() for s in sent_re.split(text) if s.strip()]
            for s in sentences:
                s_vec = kb.get_embedding(s[:1000])
                if not s_vec:
                    continue
                score = float(cosine_similarity([qvec], [s_vec])[0][0])
                cands.append((score, s, fn))
        cands.sort(key=lambda x: x[0], reverse=True)
        top = cands[:top_n]
        if not top:
            return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤"
        out = "üìå ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ (Extractive):\n"
        for i, (sc, s, fn) in enumerate(top, 1):
            out += f"- {s}  \n  ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á: [{i}] {fn} (sim={sc:.3f})\n"
        return out
    except Exception as e:
        st.error(f"‚ùå Extractive answer failed: {e}")
        return None

# ==================== UI Helpers (‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°) ====================

def ui_kb_maintenance(db_path: str):
    """‡∏Å‡∏•‡πà‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠ Re-index / Reset KB (‡πÉ‡∏ä‡πâ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏î‡∏¥‡∏°)"""
    st.header("üß∞ Knowledge Base Maintenance (Safe Add-on)")
    kb = RAGKnowledgeBaseX(db_path=db_path)

    stats = kb.get_stats()
    if "error" in stats:
        st.error(f"DB Error: {stats['error']}")
    else:
        c1, c2, c3 = st.columns(3)
        c1.metric("üìö Documents", stats["total_documents"])
        c2.metric("üìÑ Chunks", stats["total_chunks"])
        c3.metric("üí¨ Chat Sessions", stats["total_chat_sessions"])

    st.markdown("---")
    st.subheader("üîÑ Re-index documents")
    # ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
    conn = sqlite3.connect(db_path); cursor = conn.cursor()
    cursor.execute("SELECT DISTINCT filename FROM documents ORDER BY filename")
    all_fns = [r[0] for r in cursor.fetchall()]
    conn.close()
    re_docs = st.multiselect("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ (‡∏ß‡πà‡∏≤‡∏á = ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)", options=all_fns, default=[])
    colx, coly = st.columns(2)
    with colx:
        re_chunk = st.number_input("Chunk size", 400, 4000, 1200, 100)
    with coly:
        re_overlap = st.number_input("Overlap", 0, 800, 120, 10)

    if st.button("üîÅ Re-index Now"):
        with st.spinner("Re-indexing‚Ä¶"):
            kb.reindex_documents(filenames=re_docs if re_docs else None, chunk_size=int(re_chunk), overlap=int(re_overlap))
        st.success("‚úÖ Re-index completed")
        st.experimental_rerun()

    st.markdown("---")
    st.subheader("üóëÔ∏è Reset Knowledge Base (‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)")
    confirm1 = st.checkbox("‡∏â‡∏±‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
    confirm2 = st.text_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ RESET ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô")
    if st.button("‚ùó Reset KB", type="primary", disabled=not (confirm1 and confirm2.strip().upper() == "RESET")):
        kb.reset_kb()
        st.success("‚úÖ Knowledge base reset")
        st.experimental_rerun()


def ui_enhanced_ai_chat(
    db_path: str,
    ollama_api_url: str = OLLAMA_API_URL_DEFAULT,
    answer_models: Optional[List[str]] = None,
    session_key_chat: str = "enhanced_chat_history"
):
    """‡∏´‡∏ô‡πâ‡∏≤ Chat ‡πÄ‡∏™‡∏£‡∏¥‡∏°: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Embedding, Top‚ÄëK, MMR, Extractive/Strict, ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á"""
    st.header("üí¨ Enhanced AI Chat (Safe Add-on)")

    if session_key_chat not in st.session_state:
        st.session_state[session_key_chat] = []

    kb = RAGKnowledgeBaseX(db_path=db_path)

    # Controls
    st.subheader("‚öôÔ∏è Retrieval & Answer Settings")
    c1, c2 = st.columns(2)
    with c1:
        embedding_api = st.text_input("Embedding API URL", EMBEDDING_API_URL_DEFAULT)
    with c2:
        embedding_model = st.selectbox("Embedding model", ["nomic-embed-text:latest"], index=0)

    kb.set_embedding_api(embedding_api)
    kb.set_embedding_model(embedding_model)

    r1, r2, r3 = st.columns(3)
    with r1:
        top_k = st.slider("Top‚ÄëK", 3, 10, 5)
    with r2:
        use_mmr = st.checkbox("Use MMR diversity", value=True)
    with r3:
        lambda_mult = st.slider("MMR Œª (relevance‚Üîdiversity)", 0.5, 0.9, 0.7, 0.05)

    m1, m2, m3 = st.columns(3)
    with m1:
        extractive_mode = st.checkbox("Extractive answer (quotes)", value=False)
    with m2:
        strict_mode = st.checkbox("Strict LLM (no guessing + citations)", value=True)
    with m3:
        model_choice = st.selectbox(
            "LLM model for generation",
            answer_models or ["qwen2.5:14b", "scb10x/llama3.1-typhoon2-8b-instruct:latest", "scb10x/typhoon-ocr-7b:latest"],
            index=0
        )

    stats = kb.get_stats()
    if stats.get("total_documents", 0) == 0:
        st.warning("‚ö†Ô∏è ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏ô Knowledge Base ‚Äî ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡πà‡∏≠‡∏ô")
        return

    st.markdown("---")
    st.subheader("üó£Ô∏è Ask a question from your documents")
    # show history
    for i, (q, a, ctx) in enumerate(st.session_state[session_key_chat]):
        st.markdown(f'<div style="padding:0.75rem;border-left:4px solid #2196f3;background:#e3f2fd;border-radius:8px;margin-bottom:4px;"><b>‡∏Ñ‡∏∏‡∏ì:</b><br>{q}</div>', unsafe_allow_html=True)
        st.markdown(f'<div style="padding:0.75rem;border-left:4px solid #9c27b0;background:#f3e5f5;border-radius:8px;margin-bottom:8px;"><b>‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢:</b><br>{a}</div>', unsafe_allow_html=True)
        with st.expander(f"üìö Sources (Chat {i+1})"):
            for j, (fn, content, sim) in enumerate(ctx):
                st.markdown(f"**{j+1}. {fn}** (sim={sim:.3f})")
                st.code((content[:400] + "‚Ä¶") if len(content) > 400 else content, language="markdown")

    with st.form("enhanced_chat_form", clear_on_submit=True):
        user_q = st.text_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°", placeholder="‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏™‡∏£‡∏∏‡∏õ‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Ø ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á")
        cc1, cc2 = st.columns([1,1])
        with cc1:
            ask_btn = st.form_submit_button("üöÄ Ask")
        with cc2:
            clear_btn = st.form_submit_button("üóëÔ∏è Clear Chat")

    if clear_btn:
        st.session_state[session_key_chat] = []
        st.experimental_rerun()

    if ask_btn and user_q.strip():
        with st.spinner("üîç Retrieving & answering‚Ä¶"):
            ctx_docs = kb.search_similar(user_q, top_k=top_k, use_mmr=use_mmr, lambda_mult=lambda_mult)

            if not ctx_docs:
                st.warning("‚ö†Ô∏è No relevant information found in knowledge base")
                return

            answer_text = None
            if extractive_mode:
                answer_text = extractive_answer(user_q, ctx_docs, kb, top_n=3)

            if (not answer_text or not answer_text.strip()) and strict_mode:
                answer_text = generate_rag_response_strict(user_q, ctx_docs, llm_model=model_choice, ollama_api_url=OLLAMA_API_URL_DEFAULT)

            # fallback (‡πÑ‡∏°‡πà‡∏õ‡∏¥‡∏î‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏õ‡∏¥‡∏î strict ‡πÅ‡∏ï‡∏∞ extractive)
            if not answer_text or not answer_text.strip():
                answer_text = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤"

            st.session_state[session_key_chat].append((user_q, answer_text, ctx_docs[:3]))

            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÄ‡∏î‡∏¥‡∏° (session_id ‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô 'enhanced')
            try:
                conn = sqlite3.connect(db_path); cursor = conn.cursor()
                cursor.execute('''
                    INSERT INTO chat_sessions (session_id, question, answer, context)
                    VALUES (?, ?, ?, ?)
                ''', ("enhanced", user_q, answer_text,
                      json.dumps([(d[0], d[1][:200], d[2]) for d in ctx_docs[:3]], ensure_ascii=False)))
                conn.commit(); conn.close()
            except Exception as e:
                st.warning(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏ä‡∏ó‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")

            st.experimental_rerun()
